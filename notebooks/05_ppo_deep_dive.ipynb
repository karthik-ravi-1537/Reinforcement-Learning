{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO: The Algorithm Behind RLHF\n",
    "\n",
    "**Goal**: Deeply understand PPO — the most important algorithm for LLM alignment.\n",
    "\n",
    "## Why PPO Matters\n",
    "\n",
    "PPO is used in:\n",
    "- **InstructGPT / ChatGPT**: Fine-tuning GPT with human preferences\n",
    "- **Claude**: Anthropic's alignment training\n",
    "- **Game AI**: OpenAI Five (Dota 2), robotic control\n",
    "\n",
    "It's the default RL algorithm for RLHF because it's **stable, simple, and works well**.\n",
    "\n",
    "## PPO in One Sentence\n",
    "\n",
    "> Update the policy to maximize reward, but **don't change it too much** in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rl_lab.agents.ppo import PPOAgent, ActorCritic\n",
    "from rl_lab.utils.common import get_device\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Evolution: REINFORCE → A2C → TRPO → PPO\n",
    "\n",
    "### REINFORCE\n",
    "$$\\nabla J = \\mathbb{E}[\\nabla \\log \\pi(a|s) \\cdot G_t]$$\n",
    "Problem: High variance, unstable\n",
    "\n",
    "### A2C (Advantage Actor-Critic)\n",
    "$$\\nabla J = \\mathbb{E}[\\nabla \\log \\pi(a|s) \\cdot A(s,a)]$$\n",
    "Better: Use advantage, TD updates. But gradient steps can be too large.\n",
    "\n",
    "### TRPO (Trust Region Policy Optimization)\n",
    "Constrain KL divergence: $D_{KL}(\\pi_{old} || \\pi_{new}) \\leq \\delta$\n",
    "Works but complex (conjugate gradient, line search).\n",
    "\n",
    "### PPO: TRPO Made Simple\n",
    "Instead of a hard KL constraint, **clip the objective**:\n",
    "\n",
    "$$L^{CLIP} = \\mathbb{E}\\left[\\min\\left(r(\\theta) A, \\text{clip}(r(\\theta), 1-\\varepsilon, 1+\\varepsilon) A\\right)\\right]$$\n",
    "\n",
    "Where $r(\\theta) = \\frac{\\pi_{new}(a|s)}{\\pi_{old}(a|s)}$ is the probability ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Understanding the Clipped Objective\n",
    "\n",
    "The clip prevents the policy from changing too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clipped objective\n",
    "clip_eps = 0.2\n",
    "r = np.linspace(0, 2.5, 500)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Case 1: Positive advantage (good action, want to increase probability)\n",
    "A_pos = 1.0\n",
    "surr1_pos = r * A_pos\n",
    "surr2_pos = np.clip(r, 1 - clip_eps, 1 + clip_eps) * A_pos\n",
    "objective_pos = np.minimum(surr1_pos, surr2_pos)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(r, surr1_pos, '--', label='Unclipped (r·A)', alpha=0.7)\n",
    "ax.plot(r, surr2_pos, '--', label='Clipped', alpha=0.7)\n",
    "ax.plot(r, objective_pos, 'k-', linewidth=2, label='PPO objective')\n",
    "ax.axvline(x=1, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axvspan(1-clip_eps, 1+clip_eps, alpha=0.1, color='green', label='Trust region')\n",
    "ax.set_xlabel('Probability ratio r(θ)')\n",
    "ax.set_ylabel('Objective')\n",
    "ax.set_title('Positive Advantage (A > 0)\\n\"Good action: increase prob, but not too much\"')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Case 2: Negative advantage (bad action, want to decrease probability)\n",
    "A_neg = -1.0\n",
    "surr1_neg = r * A_neg\n",
    "surr2_neg = np.clip(r, 1 - clip_eps, 1 + clip_eps) * A_neg\n",
    "objective_neg = np.minimum(surr1_neg, surr2_neg)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(r, surr1_neg, '--', label='Unclipped (r·A)', alpha=0.7)\n",
    "ax.plot(r, surr2_neg, '--', label='Clipped', alpha=0.7)\n",
    "ax.plot(r, objective_neg, 'k-', linewidth=2, label='PPO objective')\n",
    "ax.axvline(x=1, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axvspan(1-clip_eps, 1+clip_eps, alpha=0.1, color='green', label='Trust region')\n",
    "ax.set_xlabel('Probability ratio r(θ)')\n",
    "ax.set_ylabel('Objective')\n",
    "ax.set_title('Negative Advantage (A < 0)\\n\"Bad action: decrease prob, but not too much\"')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: The clip creates a 'trust region' around r=1.\")\n",
    "print(\"The policy can only change by ±ε per update step.\")\n",
    "print(f\"With ε={clip_eps}, ratio stays in [{1-clip_eps}, {1+clip_eps}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Generalized Advantage Estimation (GAE)\n",
    "\n",
    "How do we estimate the advantage $A(s,a)$?\n",
    "\n",
    "**One-step TD**: $A_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "- Low variance, high bias\n",
    "\n",
    "**Monte Carlo**: $A_t = G_t - V(s_t)$  where $G_t = \\sum r$\n",
    "- High variance, low bias\n",
    "\n",
    "**GAE** interpolates with parameter $\\lambda$:\n",
    "\n",
    "$$A^{GAE}_t = \\sum_{l=0}^{T-t} (\\gamma\\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "where $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error.\n",
    "\n",
    "| λ | Behavior |\n",
    "|---|----------|\n",
    "| 0 | One-step TD (high bias, low variance) |\n",
    "| 0.95 | Good default |\n",
    "| 1 | Monte Carlo (low bias, high variance) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. The Full PPO Algorithm\n",
    "\n",
    "```\n",
    "for iteration = 1, 2, ...:\n",
    "    # 1. Collect rollout\n",
    "    Run policy π_old for T timesteps, store (s, a, r, done, log_prob, value)\n",
    "\n",
    "    # 2. Compute advantages\n",
    "    Compute GAE advantages A_t and returns R_t\n",
    "\n",
    "    # 3. PPO update (multiple epochs!)\n",
    "    for epoch = 1 to K:\n",
    "        for mini-batch in shuffle(rollout):\n",
    "            r(θ) = π_new(a|s) / π_old(a|s)\n",
    "            L_clip = min(r·A, clip(r, 1-ε, 1+ε)·A)\n",
    "            L_value = MSE(V(s), R_t)\n",
    "            L_entropy = -H(π)\n",
    "            Loss = -L_clip + c1·L_value - c2·L_entropy\n",
    "            Update θ with gradient descent\n",
    "```\n",
    "\n",
    "**Three losses combined**:\n",
    "1. **Policy loss** (clipped): Improve the policy\n",
    "2. **Value loss**: Train the critic\n",
    "3. **Entropy bonus**: Prevent premature convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Train PPO on CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env_id=\"CartPole-v1\", n_iterations=200, rollout_steps=2048, **kwargs):\n",
    "    \"\"\"\n",
    "    Train PPO and return metrics.\n",
    "    \n",
    "    Args:\n",
    "        env_id: Gymnasium environment ID\n",
    "        n_iterations: Number of rollout+update cycles\n",
    "        rollout_steps: Steps to collect before each update\n",
    "        **kwargs: PPO hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (agent, metrics dict)\n",
    "    \"\"\"\n",
    "    env = gym.make(env_id)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    agent = PPOAgent(obs_dim=obs_dim, n_actions=n_actions, device=device, **kwargs)\n",
    "    \n",
    "    all_rewards = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Collect rollout\n",
    "        for step in range(rollout_steps):\n",
    "            action, log_prob, value = agent.select_action(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_transition(state, action, reward, done, log_prob, value)\n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                all_rewards.append(ep_reward)\n",
    "                ep_reward = 0\n",
    "                state, _ = env.reset()\n",
    "        \n",
    "        # PPO update\n",
    "        metrics = agent.update()\n",
    "        policy_losses.append(metrics['policy_loss'])\n",
    "        value_losses.append(metrics['value_loss'])\n",
    "        entropies.append(metrics['entropy'])\n",
    "        \n",
    "        if (iteration + 1) % 20 == 0:\n",
    "            recent = all_rewards[-20:] if len(all_rewards) >= 20 else all_rewards\n",
    "            avg = np.mean(recent) if recent else 0\n",
    "            print(f\"Iter {iteration+1}: avg reward = {avg:.1f}, \"\n",
    "                  f\"policy_loss = {metrics['policy_loss']:.4f}, \"\n",
    "                  f\"entropy = {metrics['entropy']:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, {\n",
    "        'rewards': all_rewards,\n",
    "        'policy_losses': policy_losses,\n",
    "        'value_losses': value_losses,\n",
    "        'entropies': entropies,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PPO on CartPole\n",
    "ppo_agent, ppo_metrics = train_ppo(\n",
    "    env_id=\"CartPole-v1\",\n",
    "    n_iterations=100,\n",
    "    rollout_steps=2048,\n",
    "    hidden_dim=64,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_eps=0.2,\n",
    "    n_epochs=4,\n",
    "    batch_size=64,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PPO training metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Rewards\n",
    "ax = axes[0, 0]\n",
    "ax.plot(ppo_metrics['rewards'], alpha=0.3, color='blue')\n",
    "window = 20\n",
    "if len(ppo_metrics['rewards']) > window:\n",
    "    rolling = np.convolve(ppo_metrics['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(ppo_metrics['rewards'])), rolling, color='red')\n",
    "ax.axhline(y=500, color='green', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Episode Rewards')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Policy loss\n",
    "ax = axes[0, 1]\n",
    "ax.plot(ppo_metrics['policy_losses'])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Policy Loss')\n",
    "ax.set_title('Policy Loss (should decrease)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Value loss\n",
    "ax = axes[1, 0]\n",
    "ax.plot(ppo_metrics['value_losses'])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Value Loss')\n",
    "ax.set_title('Value Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropy\n",
    "ax = axes[1, 1]\n",
    "ax.plot(ppo_metrics['entropies'])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Entropy')\n",
    "ax.set_title('Policy Entropy (exploration)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. PPO on LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_agent, ll_metrics = train_ppo(\n",
    "    env_id=\"LunarLander-v3\",\n",
    "    n_iterations=200,\n",
    "    rollout_steps=2048,\n",
    "    hidden_dim=128,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_eps=0.2,\n",
    "    n_epochs=4,\n",
    "    batch_size=64,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(ll_metrics['rewards'], alpha=0.2, color='blue')\n",
    "window = 30\n",
    "if len(ll_metrics['rewards']) > window:\n",
    "    rolling = np.convolve(ll_metrics['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(ll_metrics['rewards'])), rolling, color='red', label=f'{window}-ep avg')\n",
    "ax.axhline(y=200, color='green', linestyle='--', alpha=0.5, label='Solved (200)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('PPO on LunarLander-v3')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Ablation: Clipping Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different clipping values\n",
    "clip_results = {}\n",
    "for eps in [0.1, 0.2, 0.3, 0.5]:\n",
    "    print(f\"\\nTraining with clip_eps={eps}...\")\n",
    "    _, metrics = train_ppo(\n",
    "        env_id=\"CartPole-v1\",\n",
    "        n_iterations=60,\n",
    "        rollout_steps=2048,\n",
    "        clip_eps=eps,\n",
    "        seed=42,\n",
    "    )\n",
    "    clip_results[f\"ε={eps}\"] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "window = 20\n",
    "for label, data in clip_results.items():\n",
    "    if len(data['rewards']) > window:\n",
    "        rolling = np.convolve(data['rewards'], np.ones(window)/window, mode='valid')\n",
    "        ax.plot(rolling, label=label, alpha=0.8)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel(f'Reward ({window}-ep avg)')\n",
    "ax.set_title('Ablation: PPO Clipping Parameter')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. PPO → RLHF: The Connection\n",
    "\n",
    "In standard RL (what we just did):\n",
    "```\n",
    "Policy:  π(action | state)         → neural net playing CartPole\n",
    "Reward:  game score                 → +1 for staying alive\n",
    "PPO:     update π to maximize reward\n",
    "```\n",
    "\n",
    "In RLHF (aligning language models):\n",
    "```\n",
    "Policy:  π(next_token | context)    → the language model!\n",
    "Reward:  reward_model(response)     → trained from human preferences\n",
    "PPO:     update LM to maximize reward model score\n",
    "         + KL penalty to stay close to original model\n",
    "```\n",
    "\n",
    "### The RLHF Objective\n",
    "\n",
    "$$\\max_\\theta \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta(y|x)} \\left[ R(x, y) - \\beta \\cdot D_{KL}(\\pi_\\theta || \\pi_{ref}) \\right]$$\n",
    "\n",
    "- $R(x, y)$: Reward model score for response $y$ to prompt $x$\n",
    "- $D_{KL}$: KL divergence from reference (pre-trained) model\n",
    "- $\\beta$: Controls how far the model can drift from the original\n",
    "\n",
    "The KL penalty is like PPO's clipping — **prevent the policy from changing too drastically**.\n",
    "\n",
    "### What Changes Between RL and RLHF?\n",
    "\n",
    "| RL (CartPole) | RLHF (Language Model) |\n",
    "|---------------|----------------------|\n",
    "| State = observation | State = token context |\n",
    "| Action = discrete choice | Action = next token (vocab ~50k) |\n",
    "| Small network (128 hidden) | Billions of parameters |\n",
    "| Env gives reward | Reward model gives reward |\n",
    "| Clip ratio | Clip ratio + KL penalty |\n",
    "\n",
    "**The core PPO algorithm is the same!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **PPO clips the policy update** to prevent catastrophic changes\n",
    "\n",
    "2. **GAE** balances bias-variance in advantage estimation\n",
    "\n",
    "3. **Multiple mini-batch epochs** per rollout → data efficient\n",
    "\n",
    "4. **Entropy bonus** keeps exploration alive\n",
    "\n",
    "5. **PPO = the bridge to RLHF**. Same algorithm, different policy (LM) and reward (reward model)\n",
    "\n",
    "---\n",
    "## Next: Phase 3 — RLHF, DPO, and GRPO\n",
    "\n",
    "Now that we understand PPO, we can explore:\n",
    "- **RLHF**: PPO + reward model on language models\n",
    "- **DPO**: Skip the reward model entirely\n",
    "- **GRPO**: Group-based advantages (DeepSeek's approach)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
