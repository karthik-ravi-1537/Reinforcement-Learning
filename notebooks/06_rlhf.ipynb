{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF: Reinforcement Learning from Human Feedback\n",
    "\n",
    "**Goal**: Understand the full RLHF pipeline that aligns language models.\n",
    "\n",
    "## The Three Steps of RLHF\n",
    "\n",
    "```\n",
    "Step 1: Supervised Fine-Tuning (SFT)\n",
    "  Pretrained LM → Fine-tune on high-quality demonstrations → SFT Model\n",
    "\n",
    "Step 2: Reward Modeling\n",
    "  Collect human comparisons (response A > response B)\n",
    "  Train a reward model to predict human preferences\n",
    "\n",
    "Step 3: RL Fine-Tuning (PPO)\n",
    "  Use PPO to optimize the SFT model against the reward model\n",
    "  + KL penalty to prevent drifting too far from SFT model\n",
    "```\n",
    "\n",
    "We'll implement a **toy version** of each step to build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Toy Setup: Bandit Language Model\n",
    "\n",
    "To focus on the RLHF mechanics without the complexity of a full LM, we'll use:\n",
    "\n",
    "- **\"Language model\"**: A policy that picks from a vocabulary of 10 \"tokens\"\n",
    "- **\"Prompt\"**: A context vector (one of 5 possible contexts)\n",
    "- **\"Response\"**: A single token choice\n",
    "- **Hidden preference**: Humans prefer certain tokens in certain contexts\n",
    "\n",
    "This is a **contextual bandit** — the simplest version of the RLHF problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "N_CONTEXTS = 5\n",
    "VOCAB_SIZE = 10\n",
    "CONTEXT_DIM = 8\n",
    "\n",
    "# Fixed context embeddings (like prompt embeddings)\n",
    "context_embeddings = torch.randn(N_CONTEXTS, CONTEXT_DIM)\n",
    "\n",
    "# Hidden \"true\" human preferences: for each context, some tokens are preferred\n",
    "# This simulates the unknown reward function that RLHF tries to learn\n",
    "true_preferences = torch.randn(N_CONTEXTS, VOCAB_SIZE)\n",
    "true_preferences = torch.softmax(true_preferences * 2, dim=-1)  # Sharper preferences\n",
    "\n",
    "def get_true_reward(context_id: int, token_id: int) -> float:\n",
    "    \"\"\"The hidden human preference (oracle). Higher = more preferred.\"\"\"\n",
    "    return true_preferences[context_id, token_id].item()\n",
    "\n",
    "def get_human_comparison(context_id: int, token_a: int, token_b: int) -> int:\n",
    "    \"\"\"\n",
    "    Simulate a human comparison (Bradley-Terry model).\n",
    "    \n",
    "    Returns 0 if token_a preferred, 1 if token_b preferred.\n",
    "    \"\"\"\n",
    "    r_a = get_true_reward(context_id, token_a)\n",
    "    r_b = get_true_reward(context_id, token_b)\n",
    "    # Bradley-Terry: P(a > b) = exp(r_a) / (exp(r_a) + exp(r_b))\n",
    "    prob_a = np.exp(r_a) / (np.exp(r_a) + np.exp(r_b))\n",
    "    return 0 if np.random.random() < prob_a else 1\n",
    "\n",
    "print(\"True preferences for context 0:\")\n",
    "for i in range(VOCAB_SIZE):\n",
    "    bar = '█' * int(true_preferences[0, i].item() * 50)\n",
    "    print(f\"  Token {i}: {true_preferences[0, i]:.3f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Step 1: SFT Model (Pre-trained Policy)\n",
    "\n",
    "In real RLHF, this is a language model fine-tuned on demonstrations.\n",
    "Here, we train a simple policy on some \"demonstration\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyLM(nn.Module):\n",
    "    \"\"\"Tiny 'language model': maps context → token distribution.\"\"\"\n",
    "    \n",
    "    def __init__(self, context_dim: int, vocab_size: int, hidden_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(context_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, context):\n",
    "        \"\"\"Return logits over vocabulary.\"\"\"\n",
    "        return self.net(context)\n",
    "    \n",
    "    def get_distribution(self, context):\n",
    "        \"\"\"Return token probability distribution.\"\"\"\n",
    "        return Categorical(logits=self.forward(context))\n",
    "    \n",
    "    def generate(self, context_id: int) -> int:\n",
    "        \"\"\"Sample a token for a given context.\"\"\"\n",
    "        context = context_embeddings[context_id].unsqueeze(0)\n",
    "        dist = self.get_distribution(context)\n",
    "        return dist.sample().item()\n",
    "\n",
    "\n",
    "# Create SFT model (random initialization = \"pre-trained\" model)\n",
    "sft_model = ToyLM(CONTEXT_DIM, VOCAB_SIZE)\n",
    "\n",
    "# Train SFT on some demonstration data\n",
    "# (In practice: high-quality human-written responses)\n",
    "sft_optimizer = optim.Adam(sft_model.parameters(), lr=1e-2)\n",
    "\n",
    "# Generate demo data: sample good tokens based on true preferences\n",
    "demo_contexts = []\n",
    "demo_tokens = []\n",
    "for _ in range(500):\n",
    "    ctx = np.random.randint(N_CONTEXTS)\n",
    "    # Sample from true preferences (demonstrations)\n",
    "    token = torch.multinomial(true_preferences[ctx], 1).item()\n",
    "    demo_contexts.append(ctx)\n",
    "    demo_tokens.append(token)\n",
    "\n",
    "# Train SFT\n",
    "for epoch in range(100):\n",
    "    indices = np.random.permutation(len(demo_contexts))\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(indices), 32):\n",
    "        batch_idx = indices[i:i+32]\n",
    "        contexts = context_embeddings[torch.LongTensor([demo_contexts[j] for j in batch_idx])]\n",
    "        targets = torch.LongTensor([demo_tokens[j] for j in batch_idx])\n",
    "        \n",
    "        logits = sft_model(contexts)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        sft_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        sft_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "print(\"SFT training complete\")\n",
    "\n",
    "# Evaluate SFT model\n",
    "sft_rewards = []\n",
    "for ctx in range(N_CONTEXTS):\n",
    "    rewards = [get_true_reward(ctx, sft_model.generate(ctx)) for _ in range(100)]\n",
    "    sft_rewards.append(np.mean(rewards))\n",
    "print(f\"SFT avg reward: {np.mean(sft_rewards):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Step 2: Reward Model Training\n",
    "\n",
    "Train a reward model from **pairwise comparisons** (not absolute scores).\n",
    "\n",
    "### Bradley-Terry Model\n",
    "\n",
    "$$P(y_w \\succ y_l | x) = \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$$\n",
    "\n",
    "Where:\n",
    "- $y_w$ = preferred (winner) response\n",
    "- $y_l$ = less preferred (loser) response\n",
    "- $r_\\theta$ = reward model\n",
    "- $\\sigma$ = sigmoid function\n",
    "\n",
    "**Loss**: $\\mathcal{L} = -\\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward model: maps (context, token) → scalar reward.\"\"\"\n",
    "    \n",
    "    def __init__(self, context_dim: int, vocab_size: int, hidden_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(context_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, context, token_ids):\n",
    "        \"\"\"\n",
    "        Predict reward for (context, token) pairs.\n",
    "        \n",
    "        Args:\n",
    "            context: Context embeddings [batch, context_dim]\n",
    "            token_ids: Token indices [batch]\n",
    "        \n",
    "        Returns:\n",
    "            Reward predictions [batch]\n",
    "        \"\"\"\n",
    "        token_emb = self.token_embed(token_ids)\n",
    "        combined = torch.cat([context, token_emb], dim=-1)\n",
    "        return self.net(combined).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect comparison data from \"humans\"\n",
    "comparison_data = []  # (context_id, preferred_token, rejected_token)\n",
    "\n",
    "for _ in range(2000):\n",
    "    ctx = np.random.randint(N_CONTEXTS)\n",
    "    token_a = np.random.randint(VOCAB_SIZE)\n",
    "    token_b = np.random.randint(VOCAB_SIZE)\n",
    "    if token_a == token_b:\n",
    "        continue\n",
    "    \n",
    "    # Ask \"human\" which is better\n",
    "    winner = get_human_comparison(ctx, token_a, token_b)\n",
    "    if winner == 0:\n",
    "        comparison_data.append((ctx, token_a, token_b))  # a preferred\n",
    "    else:\n",
    "        comparison_data.append((ctx, token_b, token_a))  # b preferred\n",
    "\n",
    "print(f\"Collected {len(comparison_data)} comparisons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train reward model\n",
    "reward_model = RewardModel(CONTEXT_DIM, VOCAB_SIZE)\n",
    "rm_optimizer = optim.Adam(reward_model.parameters(), lr=1e-3)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(200):\n",
    "    indices = np.random.permutation(len(comparison_data))\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, len(indices), 64):\n",
    "        batch_idx = indices[i:i+64]\n",
    "        batch = [comparison_data[j] for j in batch_idx]\n",
    "        \n",
    "        ctx_ids = torch.LongTensor([b[0] for b in batch])\n",
    "        win_ids = torch.LongTensor([b[1] for b in batch])\n",
    "        lose_ids = torch.LongTensor([b[2] for b in batch])\n",
    "        contexts = context_embeddings[ctx_ids]\n",
    "        \n",
    "        # Bradley-Terry loss\n",
    "        r_win = reward_model(contexts, win_ids)\n",
    "        r_lose = reward_model(contexts, lose_ids)\n",
    "        loss = -F.logsigmoid(r_win - r_lose).mean()\n",
    "        \n",
    "        rm_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        rm_optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    losses.append(epoch_loss / n_batches)\n",
    "\n",
    "print(f\"Final RM loss: {losses[-1]:.4f}\")\n",
    "\n",
    "# Plot RM training\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Bradley-Terry Loss')\n",
    "plt.title('Reward Model Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate: does the reward model match true preferences?\n",
    "print(\"Reward Model vs True Preferences (Context 0):\\n\")\n",
    "print(f\"{'Token':>6}  {'True Pref':>10}  {'RM Score':>10}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ctx = context_embeddings[0].unsqueeze(0).expand(VOCAB_SIZE, -1)\n",
    "    tokens = torch.arange(VOCAB_SIZE)\n",
    "    rm_scores = reward_model(ctx, tokens).numpy()\n",
    "\n",
    "true_prefs = true_preferences[0].numpy()\n",
    "for i in range(VOCAB_SIZE):\n",
    "    print(f\"{i:>6}  {true_prefs[i]:>10.4f}  {rm_scores[i]:>10.4f}\")\n",
    "\n",
    "# Check rank correlation\n",
    "from scipy.stats import spearmanr\n",
    "corr, _ = spearmanr(true_prefs, rm_scores)\n",
    "print(f\"\\nSpearman correlation: {corr:.3f} (1.0 = perfect ranking)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Step 3: PPO Fine-Tuning with KL Penalty\n",
    "\n",
    "The RLHF objective:\n",
    "\n",
    "$$\\max_\\theta \\mathbb{E}_{x, y \\sim \\pi_\\theta} \\left[ r_\\phi(x, y) - \\beta \\cdot D_{KL}(\\pi_\\theta(\\cdot|x) \\| \\pi_{ref}(\\cdot|x)) \\right]$$\n",
    "\n",
    "**Two forces in tension**:\n",
    "1. **Maximize reward model score** → generate preferred responses\n",
    "2. **Stay close to reference (SFT) model** → don't collapse or hack the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rlhf_train(\n",
    "    policy: ToyLM,\n",
    "    ref_model: ToyLM,\n",
    "    reward_model: RewardModel,\n",
    "    n_iterations: int = 500,\n",
    "    batch_size: int = 32,\n",
    "    lr: float = 1e-3,\n",
    "    kl_coef: float = 0.1,\n",
    "    clip_eps: float = 0.2,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    RLHF training loop (simplified PPO with KL penalty).\n",
    "    \n",
    "    Args:\n",
    "        policy: The model to optimize\n",
    "        ref_model: Frozen reference (SFT) model\n",
    "        reward_model: Trained reward model\n",
    "        n_iterations: Number of PPO updates\n",
    "        batch_size: Samples per update\n",
    "        lr: Learning rate\n",
    "        kl_coef: KL penalty coefficient (β)\n",
    "        clip_eps: PPO clipping parameter\n",
    "    \n",
    "    Returns:\n",
    "        Training metrics\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    reward_history = []\n",
    "    kl_history = []\n",
    "    objective_history = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Sample contexts\n",
    "        ctx_ids = torch.randint(0, N_CONTEXTS, (batch_size,))\n",
    "        contexts = context_embeddings[ctx_ids]\n",
    "        \n",
    "        # Get old policy distribution (for PPO ratio)\n",
    "        with torch.no_grad():\n",
    "            old_logits = policy(contexts)\n",
    "            old_dist = Categorical(logits=old_logits)\n",
    "        \n",
    "        # Sample tokens from current policy\n",
    "        tokens = old_dist.sample()\n",
    "        old_log_probs = old_dist.log_prob(tokens)\n",
    "        \n",
    "        # Get rewards from reward model\n",
    "        with torch.no_grad():\n",
    "            rewards = reward_model(contexts, tokens)\n",
    "        \n",
    "        # Compute KL divergence from reference model\n",
    "        with torch.no_grad():\n",
    "            ref_logits = ref_model(contexts)\n",
    "            ref_dist = Categorical(logits=ref_logits)\n",
    "        \n",
    "        # Current policy (for gradient)\n",
    "        new_logits = policy(contexts)\n",
    "        new_dist = Categorical(logits=new_logits)\n",
    "        new_log_probs = new_dist.log_prob(tokens)\n",
    "        \n",
    "        # KL(π_θ || π_ref) per sample\n",
    "        kl = torch.distributions.kl_divergence(new_dist, ref_dist)\n",
    "        \n",
    "        # RLHF reward = RM score - β * KL\n",
    "        rlhf_reward = rewards - kl_coef * kl\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (rlhf_reward - rlhf_reward.mean()) / (rlhf_reward.std() + 1e-8)\n",
    "        \n",
    "        # PPO clipped objective\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "        surr1 = ratio * advantages.detach()\n",
    "        surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages.detach()\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        reward_history.append(rewards.mean().item())\n",
    "        kl_history.append(kl.mean().item())\n",
    "        objective_history.append(rlhf_reward.mean().item())\n",
    "        \n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            print(f\"Iter {iteration+1}: reward={rewards.mean():.4f}, \"\n",
    "                  f\"KL={kl.mean():.4f}, objective={rlhf_reward.mean():.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': reward_history,\n",
    "        'kl': kl_history,\n",
    "        'objective': objective_history,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Create RLHF policy (copy of SFT model)\n",
    "rlhf_policy = copy.deepcopy(sft_model)\n",
    "# Freeze reference model\n",
    "ref_model = copy.deepcopy(sft_model)\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Run RLHF training\n",
    "print(\"Starting RLHF training...\\n\")\n",
    "rlhf_metrics = rlhf_train(\n",
    "    policy=rlhf_policy,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=reward_model,\n",
    "    n_iterations=500,\n",
    "    kl_coef=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RLHF training\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(rlhf_metrics['rewards'])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Reward Model Score')\n",
    "ax.set_title('RM Reward ↑')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(rlhf_metrics['kl'])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('KL Divergence')\n",
    "ax.set_title('KL from Reference ↓ (want low)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(rlhf_metrics['objective'])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('RLHF Objective')\n",
    "ax.set_title('Reward - β·KL (total objective) ↑')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SFT vs RLHF policy\n",
    "print(\"Performance comparison (true reward):\\n\")\n",
    "\n",
    "sft_total, rlhf_total = 0, 0\n",
    "for ctx in range(N_CONTEXTS):\n",
    "    sft_r = np.mean([get_true_reward(ctx, sft_model.generate(ctx)) for _ in range(200)])\n",
    "    rlhf_r = np.mean([get_true_reward(ctx, rlhf_policy.generate(ctx)) for _ in range(200)])\n",
    "    print(f\"Context {ctx}: SFT={sft_r:.4f}  RLHF={rlhf_r:.4f}  {'✓ improved' if rlhf_r > sft_r else '✗ worse'}\")\n",
    "    sft_total += sft_r\n",
    "    rlhf_total += rlhf_r\n",
    "\n",
    "print(f\"\\nOverall: SFT={sft_total/N_CONTEXTS:.4f}  RLHF={rlhf_total/N_CONTEXTS:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. The KL Penalty: Why It Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation: different KL coefficients\n",
    "kl_results = {}\n",
    "\n",
    "for beta in [0.0, 0.01, 0.1, 1.0]:\n",
    "    print(f\"\\nTraining with β={beta}...\")\n",
    "    policy = copy.deepcopy(sft_model)\n",
    "    metrics = rlhf_train(\n",
    "        policy=policy,\n",
    "        ref_model=ref_model,\n",
    "        reward_model=reward_model,\n",
    "        n_iterations=300,\n",
    "        kl_coef=beta,\n",
    "    )\n",
    "    kl_results[f\"β={beta}\"] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "for label, data in kl_results.items():\n",
    "    ax.plot(data['rewards'], label=label, alpha=0.8)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('RM Reward')\n",
    "ax.set_title('Reward Model Score')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "for label, data in kl_results.items():\n",
    "    ax.plot(data['kl'], label=label, alpha=0.8)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('KL from Reference')\n",
    "ax.set_title('Policy Drift (KL Divergence)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight:\")\n",
    "print(\"  β=0: Highest reward but huge KL → reward hacking!\")\n",
    "print(\"  β=1: Low KL but barely improves → too conservative\")\n",
    "print(\"  β=0.1: Good balance → improves while staying close to SFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Reward Hacking\n",
    "\n",
    "When β is too low, the policy **exploits** the reward model:\n",
    "- Finds patterns that get high RM scores but aren't actually good\n",
    "- The RM is imperfect → policy finds its blind spots\n",
    "- In practice: repetitive text, sycophantic responses, format gaming\n",
    "\n",
    "**The KL penalty prevents this** by keeping the model close to the \"sane\" SFT baseline.\n",
    "\n",
    "This is one of the key motivations for **DPO** — avoid the reward model entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **RLHF has 3 steps**: SFT → Reward Model → PPO fine-tuning\n",
    "\n",
    "2. **Reward model** learns from pairwise human comparisons using Bradley-Terry\n",
    "\n",
    "3. **PPO + KL penalty** optimizes reward while preventing drift from reference model\n",
    "\n",
    "4. **β (KL coefficient)** is critical: too low → reward hacking, too high → no improvement\n",
    "\n",
    "5. **Reward hacking** is a fundamental challenge — the policy exploits RM imperfections\n",
    "\n",
    "### RLHF Challenges\n",
    "- Requires training a separate reward model\n",
    "- PPO is complex and sensitive to hyperparameters\n",
    "- Reward hacking is hard to prevent fully\n",
    "- KL coefficient needs tuning\n",
    "\n",
    "→ **DPO solves several of these problems!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
