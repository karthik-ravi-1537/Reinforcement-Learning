{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks (DQN): Deep RL From Scratch\n",
    "\n",
    "**Goal**: Understand how neural networks replace Q-tables, and the tricks that make it work.\n",
    "\n",
    "## The Problem with Tabular Methods\n",
    "- FrozenLake: 16 states → Q-table is 16×4 = 64 values. Easy.\n",
    "- CartPole: 4 continuous values → infinite states. Can't use a table.\n",
    "- Atari: 210×160×3 pixel images → ~10^70000 possible states. Impossible.\n",
    "\n",
    "**Solution**: Approximate Q(s, a) with a neural network: Q(s, a; θ)\n",
    "\n",
    "## DQN's Three Key Innovations\n",
    "1. **Neural network** Q-function approximation\n",
    "2. **Experience replay** → break correlations between consecutive samples\n",
    "3. **Target network** → stabilize the moving target problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "from rl_lab.agents.dqn import DQNAgent, ReplayBuffer\n",
    "from rl_lab.utils.common import get_device\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why Naive Q-Learning + Neural Nets Fails\n",
    "\n",
    "If you just replace the Q-table with a neural net and train naively:\n",
    "\n",
    "### Problem 1: Correlated Samples\n",
    "Consecutive transitions (s₁→s₂→s₃) are highly correlated. Neural nets assume i.i.d. data.\n",
    "Training on correlated data → catastrophic forgetting, oscillation.\n",
    "\n",
    "**Fix**: Experience Replay → sample random mini-batches from a buffer\n",
    "\n",
    "### Problem 2: Moving Target\n",
    "We update Q(s,a) toward `r + γ max Q(s', a')`, but `max Q(s', a')` changes every update!\n",
    "It's like chasing a moving target → instability, divergence.\n",
    "\n",
    "**Fix**: Target Network → use a frozen copy for computing targets, update periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Experience Replay Visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate experience replay\n",
    "env = gym.make('CartPole-v1')\n",
    "buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "# Fill buffer with some experience\n",
    "state, _ = env.reset()\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    buffer.push(state, action, reward, next_state, terminated or truncated)\n",
    "    state = next_state\n",
    "    if terminated or truncated:\n",
    "        state, _ = env.reset()\n",
    "\n",
    "print(f\"Buffer size: {len(buffer)}\")\n",
    "\n",
    "# Sample a batch\n",
    "states, actions, rewards, next_states, dones = buffer.sample(32)\n",
    "print(f\"Batch shapes: states={states.shape}, actions={actions.shape}\")\n",
    "print(f\"\\nSample state: {states[0].numpy()}\")\n",
    "print(f\"Sample action: {actions[0].item()}\")\n",
    "print(f\"Sample reward: {rewards[0].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Train DQN on CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent, env, n_episodes=500, max_steps=500):\n",
    "    \"\"\"\n",
    "    Train DQN agent and track metrics.\n",
    "    \n",
    "    Args:\n",
    "        agent: DQNAgent instance\n",
    "        env: Gymnasium environment\n",
    "        n_episodes: Number of training episodes\n",
    "        max_steps: Max steps per episode\n",
    "    \n",
    "    Returns:\n",
    "        Dict with training history\n",
    "    \"\"\"\n",
    "    rewards_history = []\n",
    "    losses = []\n",
    "    epsilons = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        ep_losses = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            metrics = agent.update(state, action, reward, next_state, done)\n",
    "            if 'loss' in metrics:\n",
    "                ep_losses.append(metrics['loss'])\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        rewards_history.append(total_reward)\n",
    "        losses.append(np.mean(ep_losses) if ep_losses else 0)\n",
    "        epsilons.append(agent.epsilon)\n",
    "        \n",
    "        if (ep + 1) % 50 == 0:\n",
    "            avg = np.mean(rewards_history[-50:])\n",
    "            print(f\"Episode {ep+1}: avg reward = {avg:.1f}, ε = {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return {'rewards': rewards_history, 'losses': losses, 'epsilons': epsilons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train DQN agent\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "agent = DQNAgent(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    n_actions=env.action_space.n,\n",
    "    hidden_dim=128,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_min=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    buffer_size=50000,\n",
    "    batch_size=64,\n",
    "    target_update_freq=100,\n",
    "    device=device,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "results = train_dqn(agent, env, n_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Rewards\n",
    "ax = axes[0]\n",
    "ax.plot(results['rewards'], alpha=0.3, color='blue')\n",
    "window = 30\n",
    "rolling = np.convolve(results['rewards'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(results['rewards'])), rolling, color='red')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Episode Rewards')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax = axes[1]\n",
    "ax.plot(results['losses'], alpha=0.5)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon\n",
    "ax = axes[2]\n",
    "ax.plot(results['epsilons'])\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Epsilon')\n",
    "ax.set_title('Exploration Rate')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Ablation: Why Each Component Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with different target update frequencies to see the effect\n",
    "ablation_results = {}\n",
    "\n",
    "for freq_label, freq in [(\"No target net (freq=1)\", 1), (\"freq=100\", 100), (\"freq=1000\", 1000)]:\n",
    "    print(f\"\\nTraining with target update {freq_label}...\")\n",
    "    env = gym.make('CartPole-v1')\n",
    "    abl_agent = DQNAgent(\n",
    "        obs_dim=4, n_actions=2, hidden_dim=128,\n",
    "        lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995,\n",
    "        buffer_size=50000, batch_size=64,\n",
    "        target_update_freq=freq, device=device, seed=42,\n",
    "    )\n",
    "    ablation_results[freq_label] = train_dqn(abl_agent, env, n_episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "window = 30\n",
    "for label, data in ablation_results.items():\n",
    "    rolling = np.convolve(data['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax.plot(rolling, label=label, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel(f'Reward ({window}-ep avg)')\n",
    "ax.set_title('Ablation: Target Network Update Frequency')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. DQN on LunarLander (Harder Environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LunarLander: 8D observation, 4 discrete actions\n",
    "ll_env = gym.make('LunarLander-v3')\n",
    "print(f\"Obs space: {ll_env.observation_space}\")\n",
    "print(f\"Action space: {ll_env.action_space} (0=noop, 1=left, 2=main, 3=right)\")\n",
    "\n",
    "ll_agent = DQNAgent(\n",
    "    obs_dim=8, n_actions=4, hidden_dim=256,\n",
    "    lr=5e-4, gamma=0.99,\n",
    "    epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.998,\n",
    "    buffer_size=100000, batch_size=128,\n",
    "    target_update_freq=200,\n",
    "    device=device, seed=42,\n",
    ")\n",
    "\n",
    "ll_results = train_dqn(ll_agent, ll_env, n_episodes=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(ll_results['rewards'], alpha=0.2, color='blue')\n",
    "window = 50\n",
    "rolling = np.convolve(ll_results['rewards'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(ll_results['rewards'])), rolling, color='red', label=f'{window}-ep avg')\n",
    "ax.axhline(y=200, color='green', linestyle='--', alpha=0.5, label='Solved threshold (200)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('DQN on LunarLander-v3')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Key Takeaways\n",
    "\n",
    "### DQN = Q-Learning + 3 Tricks\n",
    "1. **Neural network**: Generalizes across states (no table needed)\n",
    "2. **Experience replay**: Random sampling → i.i.d. training data\n",
    "3. **Target network**: Stable targets → stable training\n",
    "\n",
    "### Limitations of DQN\n",
    "- Only works for **discrete** actions (argmax over Q-values)\n",
    "- Tends to **overestimate** Q-values (fixable with Double DQN)\n",
    "- Exploration is basic (ε-greedy)\n",
    "- No obvious way to represent stochastic policies\n",
    "\n",
    "### DQN Extensions (for reference)\n",
    "| Extension | Key Idea |\n",
    "|-----------|----------|\n",
    "| Double DQN | Separate action selection from evaluation |\n",
    "| Dueling DQN | Separate V(s) and A(s,a) streams |\n",
    "| Prioritized Replay | Sample important transitions more often |\n",
    "| Rainbow | Combine all the above |\n",
    "\n",
    "### DQN vs Policy Gradients\n",
    "| DQN | Policy Gradients |\n",
    "|-----|------------------|\n",
    "| Discrete actions only | Continuous or discrete |\n",
    "| More sample efficient | Less sample efficient |\n",
    "| Off-policy (replay buffer) | Usually on-policy |\n",
    "| No stochastic policies | Natural stochastic policies |\n",
    "\n",
    "**Next**: PPO — the best of both worlds, and the backbone of RLHF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
