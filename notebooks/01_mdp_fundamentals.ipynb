{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: MDP Fundamentals & Dynamic Programming\n",
    "\n",
    "**Goal**: Build intuition for the core RL framework before jumping to algorithms.\n",
    "\n",
    "## What we'll cover:\n",
    "1. Markov Decision Processes (MDPs)\n",
    "2. Value Functions (V and Q)\n",
    "3. Bellman Equations\n",
    "4. Policy Iteration & Value Iteration\n",
    "5. Hands-on: Solve FrozenLake with DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Markov Decision Process (MDP)\n",
    "\n",
    "An MDP is defined by the tuple $(S, A, P, R, \\gamma)$:\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|--------|\n",
    "| $S$ | Set of states |\n",
    "| $A$ | Set of actions |\n",
    "| $P(s'|s,a)$ | Transition probability |\n",
    "| $R(s,a,s')$ | Reward function |\n",
    "| $\\gamma$ | Discount factor (0 to 1) |\n",
    "\n",
    "**Key insight**: The Markov property means the future depends only on the current state, not history.\n",
    "\n",
    "### FrozenLake as an MDP\n",
    "\n",
    "```\n",
    "SFFF    S = Start\n",
    "FHFH    F = Frozen (safe)\n",
    "FFFH    H = Hole (fall, episode ends)\n",
    "HFFG    G = Goal (reward = 1)\n",
    "```\n",
    "\n",
    "- **States**: 16 grid positions (0-15)\n",
    "- **Actions**: 4 (Left, Down, Right, Up)\n",
    "- **Transitions**: Slippery! 1/3 chance each of going intended direction or perpendicular\n",
    "- **Rewards**: 1 at goal, 0 elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode='ansi')\n",
    "\n",
    "print(f\"State space: {env.observation_space.n} states\")\n",
    "print(f\"Action space: {env.action_space.n} actions\")\n",
    "print(f\"Actions: 0=Left, 1=Down, 2=Right, 3=Up\")\n",
    "print()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the transition dynamics P(s'|s,a)\n",
    "# env.P[state][action] returns list of (probability, next_state, reward, done)\n",
    "\n",
    "state = 0  # Start state\n",
    "action = 2  # Right\n",
    "\n",
    "print(f\"From state {state}, taking action {action} (Right):\")\n",
    "for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
    "    print(f\"  P={prob:.2f}: next_state={next_state}, reward={reward}, done={done}\")\n",
    "    \n",
    "print(\"\\nNotice: Slippery ice means you might go Down or Up instead!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Value Functions\n",
    "\n",
    "### State Value Function $V^\\pi(s)$\n",
    "Expected return starting from state $s$, following policy $\\pi$:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid S_0 = s\\right]$$\n",
    "\n",
    "### Action Value Function $Q^\\pi(s, a)$\n",
    "Expected return starting from state $s$, taking action $a$, then following $\\pi$:\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid S_0 = s, A_0 = a\\right]$$\n",
    "\n",
    "**Why Q is useful**: We can pick the best action by comparing Q values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Bellman Equations\n",
    "\n",
    "The **key recursive relationship** that makes RL tractable.\n",
    "\n",
    "### Bellman Expectation Equation (for a policy $\\pi$)\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V^\\pi(s') \\right]$$\n",
    "\n",
    "**In words**: Value of a state = expected immediate reward + discounted value of next states.\n",
    "\n",
    "### Bellman Optimality Equation (for optimal $V^*$)\n",
    "\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V^*(s') \\right]$$\n",
    "\n",
    "**Key insight**: Optimal policy picks the action that maximizes expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Value Iteration\n",
    "\n",
    "Iteratively apply Bellman optimality equation until convergence:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) \\left[ R + \\gamma V_k(s') \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma: float = 0.99, theta: float = 1e-8) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute optimal value function and policy using value iteration.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment with P attribute (transition dynamics)\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "    \n",
    "    Returns:\n",
    "        V: Optimal value function\n",
    "        policy: Optimal deterministic policy\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "            # Bellman optimality: take max over actions\n",
    "            action_values = []\n",
    "            for a in range(n_actions):\n",
    "                q = sum(prob * (reward + gamma * V[next_s])\n",
    "                        for prob, next_s, reward, _ in env.unwrapped.P[s][a])\n",
    "                action_values.append(q)\n",
    "            V[s] = max(action_values)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        \n",
    "        iteration += 1\n",
    "        if delta < theta:\n",
    "            print(f\"Converged after {iteration} iterations\")\n",
    "            break\n",
    "    \n",
    "    # Extract policy from value function\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    for s in range(n_states):\n",
    "        action_values = []\n",
    "        for a in range(n_actions):\n",
    "            q = sum(prob * (reward + gamma * V[next_s])\n",
    "                    for prob, next_s, reward, _ in env.unwrapped.P[s][a])\n",
    "            action_values.append(q)\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run value iteration\n",
    "V_opt, policy_opt = value_iteration(env)\n",
    "\n",
    "# Visualize the optimal value function\n",
    "print(\"Optimal Value Function (4x4 grid):\")\n",
    "print(V_opt.reshape(4, 4))\n",
    "print()\n",
    "\n",
    "# Visualize the optimal policy\n",
    "action_symbols = ['←', '↓', '→', '↑']\n",
    "policy_visual = np.array([action_symbols[a] for a in policy_opt]).reshape(4, 4)\n",
    "print(\"Optimal Policy:\")\n",
    "for row in policy_visual:\n",
    "    print(' '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_value_function(V, title=\"Value Function\"):\n",
    "    \"\"\"Plot value function as a heatmap.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    V_grid = V.reshape(4, 4)\n",
    "    im = ax.imshow(V_grid, cmap='YlOrRd')\n",
    "    \n",
    "    # Add value annotations\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            ax.text(j, i, f'{V_grid[i, j]:.2f}', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    # Mark special cells\n",
    "    # Holes: 5, 7, 11, 12 -> (1,1), (1,3), (2,3), (3,0)\n",
    "    holes = [(1, 1), (1, 3), (2, 3), (3, 0)]\n",
    "    for (i, j) in holes:\n",
    "        ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=False, edgecolor='blue', linewidth=3))\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_yticks(range(4))\n",
    "    plt.colorbar(im)\n",
    "    plt.show()\n",
    "\n",
    "visualize_value_function(V_opt, \"Optimal Value Function (Blue = Holes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Policy Iteration\n",
    "\n",
    "Alternative approach with two steps:\n",
    "1. **Policy Evaluation**: Compute $V^\\pi$ for current policy\n",
    "2. **Policy Improvement**: Greedily improve policy using $V^\\pi$\n",
    "\n",
    "Repeat until policy stops changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy: np.ndarray, gamma: float = 0.99, theta: float = 1e-8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate a policy by computing its value function.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        policy: Policy to evaluate (array of actions per state)\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "    \n",
    "    Returns:\n",
    "        V: Value function for the policy\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "            a = policy[s]\n",
    "            V[s] = sum(prob * (reward + gamma * V[next_s])\n",
    "                      for prob, next_s, reward, _ in env.unwrapped.P[s][a])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_iteration(env, gamma: float = 0.99) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Find optimal policy using policy iteration.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        gamma: Discount factor\n",
    "    \n",
    "    Returns:\n",
    "        V: Optimal value function\n",
    "        policy: Optimal policy\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    # Start with random policy\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        V = policy_evaluation(env, policy, gamma)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        for s in range(n_states):\n",
    "            old_action = policy[s]\n",
    "            \n",
    "            # Find best action\n",
    "            action_values = []\n",
    "            for a in range(n_actions):\n",
    "                q = sum(prob * (reward + gamma * V[next_s])\n",
    "                        for prob, next_s, reward, _ in env.unwrapped.P[s][a])\n",
    "                action_values.append(q)\n",
    "            policy[s] = np.argmax(action_values)\n",
    "            \n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "        \n",
    "        iteration += 1\n",
    "        if policy_stable:\n",
    "            print(f\"Policy converged after {iteration} iterations\")\n",
    "            break\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run policy iteration\n",
    "V_pi, policy_pi = policy_iteration(env)\n",
    "\n",
    "print(\"\\nPolicies match:\", np.array_equal(policy_opt, policy_pi))\n",
    "print(\"Value functions match:\", np.allclose(V_opt, V_pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Test the Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy: np.ndarray, n_episodes: int = 1000) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate a policy by running episodes and computing success rate.\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        policy: Policy to evaluate\n",
    "        n_episodes: Number of episodes to run\n",
    "    \n",
    "    Returns:\n",
    "        Success rate (fraction of episodes reaching goal)\n",
    "    \"\"\"\n",
    "    successes = 0\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            if reward == 1:\n",
    "                successes += 1\n",
    "    \n",
    "    return successes / n_episodes\n",
    "\n",
    "# Compare random vs optimal policy\n",
    "random_policy = np.random.randint(0, 4, size=16)\n",
    "\n",
    "random_success = evaluate_policy(env, random_policy)\n",
    "optimal_success = evaluate_policy(env, policy_opt)\n",
    "\n",
    "print(f\"Random policy success rate:  {random_success:.1%}\")\n",
    "print(f\"Optimal policy success rate: {optimal_success:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **MDPs** formalize sequential decision making with states, actions, transitions, rewards\n",
    "\n",
    "2. **Value functions** tell us how good states/actions are under a policy\n",
    "\n",
    "3. **Bellman equations** decompose value into immediate reward + future value (recursive structure!)\n",
    "\n",
    "4. **Value Iteration** directly finds optimal value, then extracts policy\n",
    "\n",
    "5. **Policy Iteration** alternates between evaluation and improvement\n",
    "\n",
    "### Limitations of Dynamic Programming\n",
    "- Requires **full knowledge** of P(s'|s,a) - the transition dynamics\n",
    "- Scales poorly: O(|S|²|A|) per iteration\n",
    "- Not practical for large or continuous state spaces\n",
    "\n",
    "**Next**: Q-learning and SARSA - learn without knowing the dynamics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Non-slippery FrozenLake**: Set `is_slippery=False`. How do the optimal values change?\n",
    "\n",
    "2. **Discount factor**: Try γ = 0.5, 0.9, 0.999. What happens to the value function?\n",
    "\n",
    "3. **8x8 FrozenLake**: Use `FrozenLake-v1` with `map_name=\"8x8\"`. Does it still converge?\n",
    "\n",
    "4. **Implement Q-function version**: Modify value_iteration to compute Q(s,a) instead of V(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
