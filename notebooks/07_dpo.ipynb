{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO: Direct Preference Optimization\n",
    "\n",
    "**Goal**: Understand how DPO skips the reward model and trains directly on preferences.\n",
    "\n",
    "## The Key Insight\n",
    "\n",
    "RLHF: Train reward model → Use PPO to optimize policy against it\n",
    "\n",
    "DPO: **Skip the reward model entirely**. Derive a loss that directly optimizes the policy from preference data.\n",
    "\n",
    "## Why DPO?\n",
    "\n",
    "| RLHF | DPO |\n",
    "|------|-----|\n",
    "| Train RM + run PPO (complex) | Single supervised loss (simple) |\n",
    "| RM can be exploited (reward hacking) | No RM to exploit |\n",
    "| PPO is unstable, many hyperparams | Stable, few hyperparams |\n",
    "| Need RL infrastructure | Just needs supervised training |\n",
    "\n",
    "Reference: Rafailov et al. \"Direct Preference Optimization\" (2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The DPO Derivation (Intuitive Version)\n",
    "\n",
    "### Starting Point: RLHF Objective\n",
    "\n",
    "$$\\max_\\theta \\mathbb{E}\\left[ r(x, y) - \\beta \\cdot D_{KL}(\\pi_\\theta \\| \\pi_{ref}) \\right]$$\n",
    "\n",
    "### The Closed-Form Solution\n",
    "\n",
    "It turns out the **optimal policy** for this objective has a closed form:\n",
    "\n",
    "$$\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) \\cdot \\exp\\left(\\frac{r(x,y)}{\\beta}\\right)$$\n",
    "\n",
    "### Rearranging to Get the Implicit Reward\n",
    "\n",
    "We can solve for the reward:\n",
    "\n",
    "$$r(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x)$$\n",
    "\n",
    "### The DPO Loss\n",
    "\n",
    "Substituting this into the Bradley-Terry preference model and simplifying:\n",
    "\n",
    "$$\\mathcal{L}_{DPO} = -\\mathbb{E}\\left[ \\log \\sigma\\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right]$$\n",
    "\n",
    "### In Plain English\n",
    "\n",
    "DPO says: **increase the relative probability of preferred responses (vs reference), and decrease the relative probability of rejected responses.**\n",
    "\n",
    "The `β` parameter controls how aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Same Toy Setup as RLHF Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the toy setup from notebook 06\n",
    "N_CONTEXTS = 5\n",
    "VOCAB_SIZE = 10\n",
    "CONTEXT_DIM = 8\n",
    "\n",
    "context_embeddings = torch.randn(N_CONTEXTS, CONTEXT_DIM)\n",
    "true_preferences = torch.randn(N_CONTEXTS, VOCAB_SIZE)\n",
    "true_preferences = torch.softmax(true_preferences * 2, dim=-1)\n",
    "\n",
    "def get_true_reward(context_id, token_id):\n",
    "    return true_preferences[context_id, token_id].item()\n",
    "\n",
    "def get_human_comparison(context_id, token_a, token_b):\n",
    "    r_a = get_true_reward(context_id, token_a)\n",
    "    r_b = get_true_reward(context_id, token_b)\n",
    "    prob_a = np.exp(r_a) / (np.exp(r_a) + np.exp(r_b))\n",
    "    return 0 if np.random.random() < prob_a else 1\n",
    "\n",
    "\n",
    "class ToyLM(nn.Module):\n",
    "    def __init__(self, context_dim, vocab_size, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(context_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, context):\n",
    "        return self.net(context)\n",
    "    \n",
    "    def log_probs(self, context, tokens):\n",
    "        \"\"\"Get log probabilities for specific tokens.\"\"\"\n",
    "        logits = self.forward(context)\n",
    "        return F.log_softmax(logits, dim=-1).gather(1, tokens.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    def generate(self, context_id):\n",
    "        context = context_embeddings[context_id].unsqueeze(0)\n",
    "        dist = Categorical(logits=self.forward(context))\n",
    "        return dist.sample().item()\n",
    "\n",
    "\n",
    "# Train SFT model (same as RLHF notebook)\n",
    "sft_model = ToyLM(CONTEXT_DIM, VOCAB_SIZE)\n",
    "sft_optimizer = optim.Adam(sft_model.parameters(), lr=1e-2)\n",
    "\n",
    "demo_data = []\n",
    "for _ in range(500):\n",
    "    ctx = np.random.randint(N_CONTEXTS)\n",
    "    token = torch.multinomial(true_preferences[ctx], 1).item()\n",
    "    demo_data.append((ctx, token))\n",
    "\n",
    "for epoch in range(100):\n",
    "    indices = np.random.permutation(len(demo_data))\n",
    "    for i in range(0, len(indices), 32):\n",
    "        batch = [demo_data[indices[j]] for j in range(i, min(i+32, len(indices)))]\n",
    "        contexts = context_embeddings[torch.LongTensor([b[0] for b in batch])]\n",
    "        targets = torch.LongTensor([b[1] for b in batch])\n",
    "        loss = F.cross_entropy(sft_model(contexts), targets)\n",
    "        sft_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        sft_optimizer.step()\n",
    "\n",
    "print(\"SFT model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect preference data\n",
    "preference_data = []\n",
    "for _ in range(2000):\n",
    "    ctx = np.random.randint(N_CONTEXTS)\n",
    "    token_a = np.random.randint(VOCAB_SIZE)\n",
    "    token_b = np.random.randint(VOCAB_SIZE)\n",
    "    if token_a == token_b:\n",
    "        continue\n",
    "    winner = get_human_comparison(ctx, token_a, token_b)\n",
    "    if winner == 0:\n",
    "        preference_data.append((ctx, token_a, token_b))\n",
    "    else:\n",
    "        preference_data.append((ctx, token_b, token_a))\n",
    "\n",
    "print(f\"Collected {len(preference_data)} preference pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Implementing DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(\n",
    "    policy: ToyLM,\n",
    "    ref_model: ToyLM,\n",
    "    contexts: torch.Tensor,\n",
    "    chosen_tokens: torch.Tensor,\n",
    "    rejected_tokens: torch.Tensor,\n",
    "    beta: float = 0.1,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the DPO loss.\n",
    "    \n",
    "    L_DPO = -E[log σ(β (log π(y_w|x)/π_ref(y_w|x) - log π(y_l|x)/π_ref(y_l|x)))]\n",
    "    \n",
    "    Args:\n",
    "        policy: Policy model being optimized\n",
    "        ref_model: Frozen reference model\n",
    "        contexts: Context embeddings [batch, dim]\n",
    "        chosen_tokens: Preferred token ids [batch]\n",
    "        rejected_tokens: Rejected token ids [batch]\n",
    "        beta: Temperature parameter (controls preference strength)\n",
    "    \n",
    "    Returns:\n",
    "        Scalar DPO loss\n",
    "    \"\"\"\n",
    "    # Policy log probs\n",
    "    pi_chosen = policy.log_probs(contexts, chosen_tokens)\n",
    "    pi_rejected = policy.log_probs(contexts, rejected_tokens)\n",
    "    \n",
    "    # Reference log probs\n",
    "    with torch.no_grad():\n",
    "        ref_chosen = ref_model.log_probs(contexts, chosen_tokens)\n",
    "        ref_rejected = ref_model.log_probs(contexts, rejected_tokens)\n",
    "    \n",
    "    # Log ratios\n",
    "    chosen_logratios = pi_chosen - ref_chosen\n",
    "    rejected_logratios = pi_rejected - ref_rejected\n",
    "    \n",
    "    # DPO loss\n",
    "    logits = beta * (chosen_logratios - rejected_logratios)\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "    \n",
    "    return loss, {\n",
    "        'chosen_reward': (beta * chosen_logratios).mean().item(),\n",
    "        'rejected_reward': (beta * rejected_logratios).mean().item(),\n",
    "        'reward_margin': (beta * (chosen_logratios - rejected_logratios)).mean().item(),\n",
    "        'accuracy': (logits > 0).float().mean().item(),\n",
    "    }\n",
    "\n",
    "# Quick test\n",
    "test_ctx = context_embeddings[:3]\n",
    "test_chosen = torch.LongTensor([1, 2, 3])\n",
    "test_rejected = torch.LongTensor([4, 5, 6])\n",
    "\n",
    "ref_model = copy.deepcopy(sft_model)\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "loss, metrics = dpo_loss(sft_model, ref_model, test_ctx, test_chosen, test_rejected)\n",
    "print(f\"DPO loss: {loss.item():.4f}\")\n",
    "print(f\"Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dpo(\n",
    "    sft_model: ToyLM,\n",
    "    preference_data: list,\n",
    "    beta: float = 0.1,\n",
    "    lr: float = 1e-3,\n",
    "    n_epochs: int = 50,\n",
    "    batch_size: int = 64,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train a model with DPO.\n",
    "    \n",
    "    Args:\n",
    "        sft_model: Starting (SFT) model\n",
    "        preference_data: List of (context_id, chosen_token, rejected_token)\n",
    "        beta: DPO temperature\n",
    "        lr: Learning rate\n",
    "        n_epochs: Training epochs\n",
    "        batch_size: Batch size\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (trained model, metrics history)\n",
    "    \"\"\"\n",
    "    policy = copy.deepcopy(sft_model)\n",
    "    ref_model = copy.deepcopy(sft_model)\n",
    "    for p in ref_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': [], 'reward_margin': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        indices = np.random.permutation(len(preference_data))\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_margin = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            batch = [preference_data[j] for j in batch_idx]\n",
    "            \n",
    "            ctx_ids = torch.LongTensor([b[0] for b in batch])\n",
    "            chosen = torch.LongTensor([b[1] for b in batch])\n",
    "            rejected = torch.LongTensor([b[2] for b in batch])\n",
    "            contexts = context_embeddings[ctx_ids]\n",
    "            \n",
    "            loss, metrics = dpo_loss(policy, ref_model, contexts, chosen, rejected, beta)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += metrics['accuracy']\n",
    "            epoch_margin += metrics['reward_margin']\n",
    "            n_batches += 1\n",
    "        \n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['accuracy'].append(epoch_acc / n_batches)\n",
    "        history['reward_margin'].append(epoch_margin / n_batches)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: loss={history['loss'][-1]:.4f}, \"\n",
    "                  f\"acc={history['accuracy'][-1]:.1%}, \"\n",
    "                  f\"margin={history['reward_margin'][-1]:.4f}\")\n",
    "    \n",
    "    return policy, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DPO\n",
    "print(\"Training DPO...\\n\")\n",
    "dpo_policy, dpo_history = train_dpo(\n",
    "    sft_model=sft_model,\n",
    "    preference_data=preference_data,\n",
    "    beta=0.1,\n",
    "    lr=1e-3,\n",
    "    n_epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DPO training\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(dpo_history['loss'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('DPO Loss')\n",
    "ax.set_title('Loss ↓')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(dpo_history['accuracy'])\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Preference Accuracy')\n",
    "ax.set_title('Accuracy ↑ (assigns higher reward to preferred)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(dpo_history['reward_margin'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Reward Margin')\n",
    "ax.set_title('Chosen - Rejected reward margin ↑')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SFT vs DPO\n",
    "print(\"Performance comparison (true reward):\\n\")\n",
    "\n",
    "sft_total, dpo_total = 0, 0\n",
    "for ctx in range(N_CONTEXTS):\n",
    "    sft_r = np.mean([get_true_reward(ctx, sft_model.generate(ctx)) for _ in range(200)])\n",
    "    dpo_r = np.mean([get_true_reward(ctx, dpo_policy.generate(ctx)) for _ in range(200)])\n",
    "    print(f\"Context {ctx}: SFT={sft_r:.4f}  DPO={dpo_r:.4f}  {'✓' if dpo_r > sft_r else '✗'}\")\n",
    "    sft_total += sft_r\n",
    "    dpo_total += dpo_r\n",
    "\n",
    "print(f\"\\nOverall: SFT={sft_total/N_CONTEXTS:.4f}  DPO={dpo_total/N_CONTEXTS:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. DPO β Ablation\n",
    "\n",
    "β controls how strongly the model moves away from the reference.\n",
    "- **Small β**: Aggressive optimization, can overfit to preferences\n",
    "- **Large β**: Conservative, stays close to SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_results = {}\n",
    "for beta in [0.01, 0.1, 0.5, 2.0]:\n",
    "    print(f\"\\nβ={beta}...\")\n",
    "    policy, history = train_dpo(sft_model, preference_data, beta=beta, n_epochs=50)\n",
    "    \n",
    "    # Evaluate\n",
    "    rewards = []\n",
    "    for ctx in range(N_CONTEXTS):\n",
    "        r = np.mean([get_true_reward(ctx, policy.generate(ctx)) for _ in range(200)])\n",
    "        rewards.append(r)\n",
    "    avg_reward = np.mean(rewards)\n",
    "    \n",
    "    beta_results[f\"β={beta}\"] = {'history': history, 'avg_reward': avg_reward}\n",
    "    print(f\"  Avg reward: {avg_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "for label, data in beta_results.items():\n",
    "    ax.plot(data['history']['loss'], label=label, alpha=0.8)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('DPO Loss')\n",
    "ax.set_title('DPO Loss by β')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "betas = list(beta_results.keys())\n",
    "rewards = [beta_results[b]['avg_reward'] for b in betas]\n",
    "colors = ['#e74c3c', '#2ecc71', '#3498db', '#9b59b6']\n",
    "bars = ax.bar(betas, rewards, color=colors)\n",
    "ax.set_ylabel('Average True Reward')\n",
    "ax.set_title('Final Performance by β')\n",
    "for bar, r in zip(bars, rewards):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, r + 0.002, f'{r:.4f}', ha='center')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. DPO vs RLHF: Side by Side\n",
    "\n",
    "### Conceptual Comparison\n",
    "\n",
    "| Aspect | RLHF (PPO) | DPO |\n",
    "|--------|-----------|-----|\n",
    "| **Pipeline** | SFT → RM → PPO | SFT → DPO |\n",
    "| **Reward model** | Explicit (trained separately) | Implicit (embedded in policy) |\n",
    "| **Optimization** | On-policy RL (PPO) | Supervised learning |\n",
    "| **Stability** | Sensitive to hyperparams | More stable |\n",
    "| **Compute** | High (RL rollouts) | Lower (just forward/backward) |\n",
    "| **Reward hacking** | Possible | Less likely (no explicit RM) |\n",
    "\n",
    "### The Implicit Reward\n",
    "\n",
    "DPO defines an **implicit reward** through the policy itself:\n",
    "\n",
    "$$r(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}$$\n",
    "\n",
    "Higher log-ratio = model thinks this response is better than reference would.\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "| Use DPO when... | Use RLHF when... |\n",
    "|-----------------|------------------|\n",
    "| You have preference data | You need online exploration |\n",
    "| Simplicity matters | You need an explicit reward signal |\n",
    "| Limited compute | You want to iterate on the reward model |\n",
    "| Offline training is fine | Online data collection is feasible |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. DPO Variants\n",
    "\n",
    "DPO spawned many variants:\n",
    "\n",
    "| Variant | Key Idea |\n",
    "|---------|----------|\n",
    "| **IPO** (Identity PO) | Uses identity function instead of sigmoid, more robust |\n",
    "| **KTO** (Kahneman-Tversky) | Works with just good/bad labels, no pairs needed |\n",
    "| **ORPO** (Odds Ratio PO) | Combines SFT and preference in one loss |\n",
    "| **SimPO** (Simple PO) | Reference-free, uses length-normalized log-probs |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **DPO skips the reward model** by showing the optimal policy has a closed-form solution\n",
    "\n",
    "2. **The DPO loss** directly increases probability of preferred responses relative to rejected ones\n",
    "\n",
    "3. **β controls aggressiveness**: how far the model moves from the reference\n",
    "\n",
    "4. **Simpler than RLHF**: just supervised training on preference pairs\n",
    "\n",
    "5. **Trade-off**: DPO is offline (fixed dataset), RLHF can do online exploration\n",
    "\n",
    "---\n",
    "**Next**: GRPO — DeepSeek's approach that brings back RL but simplifies the critic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
