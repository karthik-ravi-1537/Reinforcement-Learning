{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO: Group Relative Policy Optimization\n",
    "\n",
    "**Goal**: Understand DeepSeek's GRPO — a simplified RL approach that eliminates the critic.\n",
    "\n",
    "## The Problem GRPO Solves\n",
    "\n",
    "| Method | Reward Model? | Critic (V network)? | RL? |\n",
    "|--------|:---:|:---:|:---:|\n",
    "| RLHF (PPO) | Yes | Yes | Yes |\n",
    "| DPO | No | No | No |\n",
    "| **GRPO** | Yes | **No** | Yes |\n",
    "\n",
    "GRPO keeps the RL training loop (online generation) but **removes the critic network** by using group-based advantages.\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "Instead of learning V(s) to estimate advantages:\n",
    "1. For each prompt, generate a **group** of responses\n",
    "2. Score all responses with a reward model\n",
    "3. Compute **relative advantages** within the group (normalize rewards)\n",
    "4. Use PPO-style clipped updates with these group advantages\n",
    "\n",
    "Reference: DeepSeek-R1 (Shao et al., 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. GRPO vs PPO: The Core Difference\n",
    "\n",
    "### PPO Advantage Estimation\n",
    "```\n",
    "For each step t in rollout:\n",
    "    δ_t = r_t + γ V(s_{t+1}) - V(s_t)     ← needs V network!\n",
    "    A_t = Σ (γλ)^l δ_{t+l}                 ← GAE\n",
    "```\n",
    "\n",
    "### GRPO Advantage Estimation\n",
    "```\n",
    "For each prompt x:\n",
    "    Generate G responses: y_1, ..., y_G ~ π(·|x)\n",
    "    Score each: r_1, ..., r_G = RM(x, y_i)\n",
    "    Normalize: A_i = (r_i - mean(r)) / std(r)    ← no V network!\n",
    "```\n",
    "\n",
    "**GRPO uses the group itself as the baseline** instead of a learned value function.\n",
    "\n",
    "This is a form of **REINFORCE with baseline**, where the baseline is the group mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Setup (Same Toy Problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CONTEXTS = 5\n",
    "VOCAB_SIZE = 10\n",
    "CONTEXT_DIM = 8\n",
    "\n",
    "context_embeddings = torch.randn(N_CONTEXTS, CONTEXT_DIM)\n",
    "true_preferences = torch.randn(N_CONTEXTS, VOCAB_SIZE)\n",
    "true_preferences = torch.softmax(true_preferences * 2, dim=-1)\n",
    "\n",
    "def get_true_reward(context_id, token_id):\n",
    "    return true_preferences[context_id, token_id].item()\n",
    "\n",
    "\n",
    "class ToyLM(nn.Module):\n",
    "    def __init__(self, context_dim, vocab_size, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(context_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, context):\n",
    "        return self.net(context)\n",
    "    \n",
    "    def generate(self, context_id):\n",
    "        context = context_embeddings[context_id].unsqueeze(0)\n",
    "        dist = Categorical(logits=self.forward(context))\n",
    "        return dist.sample().item()\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, context_dim, vocab_size, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(context_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, context, token_ids):\n",
    "        token_emb = self.token_embed(token_ids)\n",
    "        combined = torch.cat([context, token_emb], dim=-1)\n",
    "        return self.net(combined).squeeze(-1)\n",
    "\n",
    "\n",
    "# Train SFT model\n",
    "sft_model = ToyLM(CONTEXT_DIM, VOCAB_SIZE)\n",
    "sft_opt = optim.Adam(sft_model.parameters(), lr=1e-2)\n",
    "for epoch in range(100):\n",
    "    for _ in range(20):\n",
    "        ctx_ids = torch.randint(0, N_CONTEXTS, (32,))\n",
    "        tokens = torch.stack([torch.multinomial(true_preferences[c], 1) for c in ctx_ids]).squeeze()\n",
    "        loss = F.cross_entropy(sft_model(context_embeddings[ctx_ids]), tokens)\n",
    "        sft_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        sft_opt.step()\n",
    "\n",
    "# Train reward model from comparisons\n",
    "rm = RewardModel(CONTEXT_DIM, VOCAB_SIZE)\n",
    "rm_opt = optim.Adam(rm.parameters(), lr=1e-3)\n",
    "for epoch in range(200):\n",
    "    ctx_ids = torch.randint(0, N_CONTEXTS, (64,))\n",
    "    tok_a = torch.randint(0, VOCAB_SIZE, (64,))\n",
    "    tok_b = torch.randint(0, VOCAB_SIZE, (64,))\n",
    "    contexts = context_embeddings[ctx_ids]\n",
    "    \n",
    "    r_a_true = torch.tensor([get_true_reward(c.item(), t.item()) for c, t in zip(ctx_ids, tok_a)])\n",
    "    r_b_true = torch.tensor([get_true_reward(c.item(), t.item()) for c, t in zip(ctx_ids, tok_b)])\n",
    "    \n",
    "    r_a = rm(contexts, tok_a)\n",
    "    r_b = rm(contexts, tok_b)\n",
    "    \n",
    "    # Labels: 1 if a is preferred, simulate with Bradley-Terry\n",
    "    prob_a = torch.exp(r_a_true) / (torch.exp(r_a_true) + torch.exp(r_b_true))\n",
    "    labels = (torch.rand(64) < prob_a).float()\n",
    "    \n",
    "    loss = -labels * F.logsigmoid(r_a - r_b) - (1 - labels) * F.logsigmoid(r_b - r_a)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    rm_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    rm_opt.step()\n",
    "\n",
    "print(\"SFT model and Reward model trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Implementing GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_train(\n",
    "    policy: ToyLM,\n",
    "    ref_model: ToyLM,\n",
    "    reward_model: RewardModel,\n",
    "    n_iterations: int = 500,\n",
    "    n_prompts: int = 8,\n",
    "    group_size: int = 8,\n",
    "    lr: float = 1e-3,\n",
    "    kl_coef: float = 0.05,\n",
    "    clip_eps: float = 0.2,\n",
    "    n_epochs: int = 2,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train with GRPO (Group Relative Policy Optimization).\n",
    "    \n",
    "    Args:\n",
    "        policy: Model to optimize\n",
    "        ref_model: Frozen reference model\n",
    "        reward_model: Reward model for scoring\n",
    "        n_iterations: Number of outer iterations\n",
    "        n_prompts: Number of prompts per iteration\n",
    "        group_size: Number of responses per prompt (G)\n",
    "        lr: Learning rate\n",
    "        kl_coef: KL penalty coefficient\n",
    "        clip_eps: PPO clipping parameter\n",
    "        n_epochs: PPO epochs per iteration\n",
    "    \n",
    "    Returns:\n",
    "        Training metrics\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'rewards': [], 'kl': [], 'advantages': []}\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # === Step 1: Sample prompts ===\n",
    "        prompt_ids = torch.randint(0, N_CONTEXTS, (n_prompts,))\n",
    "        \n",
    "        # === Step 2: Generate GROUP of responses per prompt ===\n",
    "        all_contexts = []\n",
    "        all_tokens = []\n",
    "        all_old_log_probs = []\n",
    "        all_advantages = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt_id in prompt_ids:\n",
    "                ctx = context_embeddings[prompt_id].unsqueeze(0).expand(group_size, -1)\n",
    "                logits = policy(ctx)\n",
    "                dist = Categorical(logits=logits)\n",
    "                \n",
    "                # Sample G responses\n",
    "                tokens = dist.sample()  # [G]\n",
    "                old_log_probs = dist.log_prob(tokens)  # [G]\n",
    "                \n",
    "                # Score with reward model\n",
    "                rewards = reward_model(ctx, tokens)  # [G]\n",
    "                \n",
    "                # === GRPO's key innovation: group-relative advantages ===\n",
    "                # Instead of V(s), use the group mean as baseline\n",
    "                advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "                \n",
    "                all_contexts.append(ctx)\n",
    "                all_tokens.append(tokens)\n",
    "                all_old_log_probs.append(old_log_probs)\n",
    "                all_advantages.append(advantages)\n",
    "        \n",
    "        # Flatten\n",
    "        contexts = torch.cat(all_contexts)       # [n_prompts * G, dim]\n",
    "        tokens = torch.cat(all_tokens)            # [n_prompts * G]\n",
    "        old_log_probs = torch.cat(all_old_log_probs)\n",
    "        advantages = torch.cat(all_advantages)\n",
    "        \n",
    "        # === Step 3: PPO-style update with group advantages ===\n",
    "        for epoch in range(n_epochs):\n",
    "            # Current policy log probs\n",
    "            new_logits = policy(contexts)\n",
    "            new_dist = Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(tokens)\n",
    "            \n",
    "            # Reference model for KL\n",
    "            with torch.no_grad():\n",
    "                ref_logits = ref_model(contexts)\n",
    "                ref_dist = Categorical(logits=ref_logits)\n",
    "            \n",
    "            # PPO ratio\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            \n",
    "            # Clipped objective\n",
    "            surr1 = ratio * advantages.detach()\n",
    "            surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages.detach()\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # KL penalty (per-token KL divergence from reference)\n",
    "            kl = torch.distributions.kl_divergence(new_dist, ref_dist).mean()\n",
    "            \n",
    "            loss = policy_loss + kl_coef * kl\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        with torch.no_grad():\n",
    "            avg_reward = reward_model(\n",
    "                contexts[:group_size], tokens[:group_size]\n",
    "            ).mean().item()\n",
    "        \n",
    "        history['rewards'].append(avg_reward)\n",
    "        history['kl'].append(kl.item())\n",
    "        history['advantages'].append(advantages.abs().mean().item())\n",
    "        \n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            print(f\"Iter {iteration+1}: reward={avg_reward:.4f}, KL={kl.item():.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRPO\n",
    "grpo_policy = copy.deepcopy(sft_model)\n",
    "ref_model = copy.deepcopy(sft_model)\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Training GRPO...\\n\")\n",
    "grpo_metrics = grpo_train(\n",
    "    policy=grpo_policy,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=rm,\n",
    "    n_iterations=500,\n",
    "    n_prompts=8,\n",
    "    group_size=8,\n",
    "    kl_coef=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GRPO training\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(grpo_metrics['rewards'])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('RM Reward ↑')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(grpo_metrics['kl'])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('KL Divergence')\n",
    "ax.set_title('KL from Reference')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(grpo_metrics['advantages'])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('|Advantage|')\n",
    "ax.set_title('Advantage Magnitude')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "print(\"Final comparison (true reward):\\n\")\n",
    "print(f\"{'Context':>8}  {'SFT':>8}  {'GRPO':>8}\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "sft_total, grpo_total = 0, 0\n",
    "for ctx in range(N_CONTEXTS):\n",
    "    sft_r = np.mean([get_true_reward(ctx, sft_model.generate(ctx)) for _ in range(200)])\n",
    "    grpo_r = np.mean([get_true_reward(ctx, grpo_policy.generate(ctx)) for _ in range(200)])\n",
    "    print(f\"{ctx:>8}  {sft_r:>8.4f}  {grpo_r:>8.4f}  {'✓' if grpo_r > sft_r else '✗'}\")\n",
    "    sft_total += sft_r\n",
    "    grpo_total += grpo_r\n",
    "\n",
    "print(f\"\\n{'Overall':>8}  {sft_total/N_CONTEXTS:>8.4f}  {grpo_total/N_CONTEXTS:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Group Size Ablation\n",
    "\n",
    "The group size G is GRPO's key hyperparameter:\n",
    "- **Small G**: Noisy advantage estimates (like REINFORCE)\n",
    "- **Large G**: Better estimates but more compute per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_results = {}\n",
    "for G in [2, 4, 8, 16]:\n",
    "    print(f\"\\nGroup size G={G}...\")\n",
    "    policy = copy.deepcopy(sft_model)\n",
    "    metrics = grpo_train(\n",
    "        policy=policy, ref_model=ref_model, reward_model=rm,\n",
    "        n_iterations=300, n_prompts=8, group_size=G,\n",
    "    )\n",
    "    group_results[f\"G={G}\"] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "window = 30\n",
    "for label, data in group_results.items():\n",
    "    smoothed = np.convolve(data['rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smoothed, label=label, alpha=0.8)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel(f'Reward ({window}-iter avg)')\n",
    "ax.set_title('GRPO: Effect of Group Size')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Larger groups → better baseline → lower variance → faster learning\")\n",
    "print(\"But: more generations per step → more compute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. GRPO + Verifiable Rewards (RLVR)\n",
    "\n",
    "GRPO is especially powerful with **verifiable rewards** — tasks where correctness can be checked automatically.\n",
    "\n",
    "### Examples\n",
    "- **Math**: Check if the final answer is correct\n",
    "- **Code**: Run tests to verify correctness\n",
    "- **Logic**: Verify logical deductions\n",
    "\n",
    "### Why GRPO + RLVR Works Well\n",
    "\n",
    "1. **Binary reward** (correct/incorrect) is easy to compute — no reward model needed!\n",
    "2. **Group sampling** naturally explores: some attempts correct, some not\n",
    "3. **Relative advantage**: \"this solution is better than average in this group\"\n",
    "\n",
    "This is how **DeepSeek-R1** was trained:\n",
    "```\n",
    "For each math problem:\n",
    "    Generate G reasoning traces with chain-of-thought\n",
    "    Check which ones get the right answer (reward = 1 or 0)\n",
    "    GRPO update: reinforce correct traces, suppress incorrect ones\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate RLVR: binary reward based on \"correctness\"\n",
    "# For each context, there's exactly one correct token\n",
    "correct_tokens = torch.argmax(true_preferences, dim=1)  # Best token per context\n",
    "\n",
    "def verifiable_reward(context_id: int, token_id: int) -> float:\n",
    "    \"\"\"Binary reward: 1 if correct, 0 otherwise.\"\"\"\n",
    "    return 1.0 if token_id == correct_tokens[context_id].item() else 0.0\n",
    "\n",
    "print(\"Correct tokens per context:\")\n",
    "for ctx in range(N_CONTEXTS):\n",
    "    print(f\"  Context {ctx}: token {correct_tokens[ctx].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_rlvr_train(\n",
    "    policy: ToyLM,\n",
    "    ref_model: ToyLM,\n",
    "    n_iterations: int = 500,\n",
    "    n_prompts: int = 8,\n",
    "    group_size: int = 16,\n",
    "    lr: float = 1e-3,\n",
    "    kl_coef: float = 0.05,\n",
    "    clip_eps: float = 0.2,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    GRPO with verifiable (binary) rewards — no reward model needed.\n",
    "    \n",
    "    Args:\n",
    "        policy: Model to optimize\n",
    "        ref_model: Frozen reference model\n",
    "        n_iterations: Training iterations\n",
    "        n_prompts: Prompts per iteration\n",
    "        group_size: Responses per prompt\n",
    "        lr: Learning rate\n",
    "        kl_coef: KL penalty\n",
    "        clip_eps: PPO clipping\n",
    "    \n",
    "    Returns:\n",
    "        Training metrics\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    history = {'accuracy': [], 'rewards': []}\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        prompt_ids = torch.randint(0, N_CONTEXTS, (n_prompts,))\n",
    "        \n",
    "        all_contexts, all_tokens, all_old_log_probs, all_advantages = [], [], [], []\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt_id in prompt_ids:\n",
    "                ctx = context_embeddings[prompt_id].unsqueeze(0).expand(group_size, -1)\n",
    "                logits = policy(ctx)\n",
    "                dist = Categorical(logits=logits)\n",
    "                tokens = dist.sample()\n",
    "                old_log_probs = dist.log_prob(tokens)\n",
    "                \n",
    "                # Binary verifiable rewards\n",
    "                rewards = torch.tensor([\n",
    "                    verifiable_reward(prompt_id.item(), t.item()) for t in tokens\n",
    "                ])\n",
    "                \n",
    "                total_correct += rewards.sum().item()\n",
    "                total_samples += group_size\n",
    "                \n",
    "                # Group-relative advantages\n",
    "                if rewards.std() > 0:\n",
    "                    advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "                else:\n",
    "                    advantages = rewards - rewards.mean()\n",
    "                \n",
    "                all_contexts.append(ctx)\n",
    "                all_tokens.append(tokens)\n",
    "                all_old_log_probs.append(old_log_probs)\n",
    "                all_advantages.append(advantages)\n",
    "        \n",
    "        contexts = torch.cat(all_contexts)\n",
    "        tokens = torch.cat(all_tokens)\n",
    "        old_log_probs = torch.cat(all_old_log_probs)\n",
    "        advantages = torch.cat(all_advantages)\n",
    "        \n",
    "        # PPO update\n",
    "        new_logits = policy(contexts)\n",
    "        new_dist = Categorical(logits=new_logits)\n",
    "        new_log_probs = new_dist.log_prob(tokens)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ref_dist = Categorical(logits=ref_model(contexts))\n",
    "        \n",
    "        ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "        surr1 = ratio * advantages.detach()\n",
    "        surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages.detach()\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        kl = torch.distributions.kl_divergence(new_dist, ref_dist).mean()\n",
    "        \n",
    "        loss = policy_loss + kl_coef * kl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        accuracy = total_correct / total_samples\n",
    "        history['accuracy'].append(accuracy)\n",
    "        history['rewards'].append(total_correct / n_prompts)\n",
    "        \n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            print(f\"Iter {iteration+1}: accuracy={accuracy:.1%}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRPO with verifiable rewards\n",
    "rlvr_policy = copy.deepcopy(sft_model)\n",
    "\n",
    "print(\"Training GRPO + RLVR...\\n\")\n",
    "rlvr_metrics = grpo_rlvr_train(\n",
    "    policy=rlvr_policy,\n",
    "    ref_model=ref_model,\n",
    "    n_iterations=500,\n",
    "    group_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "window = 30\n",
    "smoothed = np.convolve(rlvr_metrics['accuracy'], np.ones(window)/window, mode='valid')\n",
    "ax.plot(rlvr_metrics['accuracy'], alpha=0.2, color='blue')\n",
    "ax.plot(range(window-1, len(rlvr_metrics['accuracy'])), smoothed, color='red', label=f'{window}-iter avg')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Accuracy (fraction correct)')\n",
    "ax.set_title('GRPO + RLVR: Learning to Pick the Correct Token')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. The Full Landscape: Comparing All Methods\n",
    "\n",
    "| Method | Reward Source | Advantage | Online? | Complexity |\n",
    "|--------|-------------|-----------|---------|------------|\n",
    "| **RLHF (PPO)** | Reward model | GAE + critic V(s) | Yes | High |\n",
    "| **DPO** | Implicit (log ratio) | N/A (supervised) | No | Low |\n",
    "| **GRPO** | RM or verifiable | Group-relative | Yes | Medium |\n",
    "| **GRPO + RLVR** | Verifiable (exact) | Group-relative | Yes | Medium |\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Scenario | Best Method |\n",
    "|----------|-------------|\n",
    "| Have preference pairs, want simplicity | **DPO** |\n",
    "| Need online exploration, have reward model | **GRPO** |\n",
    "| Math/code tasks with verifiable answers | **GRPO + RLVR** |\n",
    "| Maximum flexibility, complex reward shaping | **RLHF (PPO)** |\n",
    "| Limited compute, offline data only | **DPO** |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **GRPO eliminates the critic** by using group-relative advantages\n",
    "\n",
    "2. **Group baseline**: mean reward in the group replaces V(s) — simpler and effective\n",
    "\n",
    "3. **Group size matters**: larger groups → lower variance → better learning\n",
    "\n",
    "4. **RLVR** combines GRPO with verifiable rewards — no reward model needed at all\n",
    "\n",
    "5. **This is how DeepSeek-R1 was trained**: GRPO + math verification = emergent reasoning\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've now covered the full journey:\n",
    "\n",
    "```\n",
    "MDPs → Q-Learning → DQN → REINFORCE → PPO → RLHF → DPO → GRPO\n",
    "```\n",
    "\n",
    "From tabular RL to the algorithms powering modern LLM alignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
