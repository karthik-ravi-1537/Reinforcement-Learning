{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE: Your First Policy Gradient\n",
    "\n",
    "**Goal**: Transition from value-based methods to directly learning a policy.\n",
    "\n",
    "## Why Policy Gradients?\n",
    "\n",
    "**Value-based** (DQN, Q-learning):\n",
    "- Learn Q(s,a), derive policy as argmax\n",
    "- Only works for discrete actions\n",
    "- Can't represent stochastic policies\n",
    "\n",
    "**Policy-based** (REINFORCE, PPO):\n",
    "- Directly parameterize and optimize the policy π(a|s; θ)\n",
    "- Works for continuous actions\n",
    "- Can learn stochastic policies\n",
    "- Foundation for RLHF (PPO fine-tunes LLM policies!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Policy Gradient Theorem\n",
    "\n",
    "We want to maximize expected return:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_t R_t\\right]$$\n",
    "\n",
    "The **policy gradient theorem** gives us:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$$\n",
    "\n",
    "Where $G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$ is the **return from time t**.\n",
    "\n",
    "### Intuition\n",
    "- $\\log \\pi(a|s)$: Direction to make action $a$ more likely in state $s$\n",
    "- $G_t$: How good was the outcome?\n",
    "- **If G_t is high**: Push policy toward that action (increase probability)\n",
    "- **If G_t is low**: Push policy away from that action (decrease probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. REINFORCE Algorithm\n",
    "\n",
    "```\n",
    "for each episode:\n",
    "    1. Collect full trajectory τ = (s₀, a₀, r₀, s₁, a₁, r₁, ...)\n",
    "    2. Compute returns Gₜ for each timestep\n",
    "    3. Compute loss = -Σ log π(aₜ|sₜ) · Gₜ\n",
    "    4. Backprop and update θ\n",
    "```\n",
    "\n",
    "**Key limitation**: Must wait for full episode (Monte Carlo). High variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Simple MLP policy network.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim: int, n_actions: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample action from policy and return (action, log_prob).\"\"\"\n",
    "        logits = self.forward(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards: list, gamma: float = 0.99) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute discounted returns for each timestep.\n",
    "    \n",
    "    G_t = r_t + γ r_{t+1} + γ² r_{t+2} + ...\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards from one episode\n",
    "        gamma: Discount factor\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of returns for each timestep\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    # Normalize returns (reduces variance significantly)\n",
    "    if len(returns) > 1:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(env_id=\"CartPole-v1\", n_episodes=1000, gamma=0.99, lr=1e-2):\n",
    "    \"\"\"\n",
    "    Train a REINFORCE agent.\n",
    "    \n",
    "    Args:\n",
    "        env_id: Gymnasium environment ID\n",
    "        n_episodes: Number of training episodes\n",
    "        gamma: Discount factor\n",
    "        lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (trained policy, reward history)\n",
    "    \"\"\"\n",
    "    env = gym.make(env_id)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    policy = PolicyNetwork(obs_dim, n_actions).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    reward_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        \n",
    "        # Collect episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_t = torch.FloatTensor(state).to(device)\n",
    "            action, log_prob = policy.get_action(state_t)\n",
    "            \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        # Compute returns and loss\n",
    "        returns = compute_returns(rewards, gamma).to(device)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        # REINFORCE loss: -E[log π(a|s) * G_t]\n",
    "        loss = -(log_probs * returns).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_reward = sum(rewards)\n",
    "        reward_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg = np.mean(reward_history[-100:])\n",
    "            print(f\"Episode {episode+1}: avg reward = {avg:.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return policy, reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train REINFORCE on CartPole\n",
    "policy, rewards = train_reinforce(n_episodes=1000, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(rewards, alpha=0.3, color='blue', label='Episode reward')\n",
    "# Rolling average\n",
    "window = 50\n",
    "rolling = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(rewards)), rolling, color='red', label=f'{window}-ep rolling avg')\n",
    "\n",
    "ax.axhline(y=500, color='green', linestyle='--', alpha=0.5, label='Max reward (CartPole)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.set_title('REINFORCE on CartPole-v1')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. REINFORCE with Baseline\n",
    "\n",
    "**Problem**: REINFORCE has high variance because returns can vary wildly.\n",
    "\n",
    "**Solution**: Subtract a baseline $b(s)$ from the returns:\n",
    "\n",
    "$$\\nabla_\\theta J = \\mathbb{E}\\left[\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot (G_t - b(s_t))\\right]$$\n",
    "\n",
    "This is **still unbiased** (the baseline cancels out in expectation) but reduces variance.\n",
    "\n",
    "Common baseline: **Value function** $V(s)$ → This gives us the **advantage**: $A(s,a) = G_t - V(s_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyWithBaseline(nn.Module):\n",
    "    \"\"\"Policy + value network (shared backbone).\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim: int, n_actions: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(hidden_dim, n_actions)\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        logits = self.policy_head(features)\n",
    "        value = self.value_head(features)\n",
    "        return logits, value.squeeze(-1)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample action and return (action, log_prob, value).\"\"\"\n",
    "        logits, value = self.forward(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce_baseline(env_id=\"CartPole-v1\", n_episodes=1000, gamma=0.99, lr=1e-2):\n",
    "    \"\"\"\n",
    "    Train REINFORCE with value baseline.\n",
    "    \n",
    "    Args:\n",
    "        env_id: Gymnasium environment ID\n",
    "        n_episodes: Number of training episodes\n",
    "        gamma: Discount factor\n",
    "        lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (trained model, reward history)\n",
    "    \"\"\"\n",
    "    env = gym.make(env_id)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    model = PolicyWithBaseline(obs_dim, n_actions).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    reward_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            state_t = torch.FloatTensor(state).to(device)\n",
    "            action, log_prob, value = model.get_action(state_t)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        returns = compute_returns(rewards, gamma).to(device)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        values = torch.stack(values)\n",
    "        \n",
    "        # Advantage = Return - Baseline(value)\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        # Policy loss: use advantage instead of raw return\n",
    "        policy_loss = -(log_probs * advantages).sum()\n",
    "        # Value loss: train baseline to predict returns\n",
    "        value_loss = nn.functional.mse_loss(values, returns)\n",
    "        \n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_reward = sum(rewards)\n",
    "        reward_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg = np.mean(reward_history[-100:])\n",
    "            print(f\"Episode {episode+1}: avg reward = {avg:.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model, reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train REINFORCE with baseline\n",
    "model_baseline, rewards_baseline = train_reinforce_baseline(n_episodes=1000, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare vanilla vs baseline\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "window = 50\n",
    "r1 = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "r2 = np.convolve(rewards_baseline, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax.plot(r1, label='REINFORCE (vanilla)', alpha=0.8)\n",
    "ax.plot(r2, label='REINFORCE + Baseline', alpha=0.8)\n",
    "ax.axhline(y=500, color='green', linestyle='--', alpha=0.5, label='Max reward')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel(f'Reward ({window}-ep avg)')\n",
    "ax.set_title('Effect of Baseline on REINFORCE')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. The Road from REINFORCE to PPO\n",
    "\n",
    "REINFORCE is the foundation. Here's how we get to PPO:\n",
    "\n",
    "| Step | Improvement | Result |\n",
    "|------|------------|--------|\n",
    "| REINFORCE | Raw policy gradient | Works but high variance |\n",
    "| + Baseline | Subtract V(s) | Lower variance |\n",
    "| + TD updates | Don't wait for full episode | Actor-Critic (A2C) |\n",
    "| + GAE | Better advantage estimation | Lower bias-variance |\n",
    "| + Trust region | Don't change policy too much | TRPO → PPO |\n",
    "\n",
    "### Why This Matters for RLHF\n",
    "\n",
    "In RLHF:\n",
    "- **Policy** = the language model π(token|context)\n",
    "- **Action** = next token to generate\n",
    "- **Reward** = reward model score of generated text\n",
    "- **PPO** optimizes the LM policy to maximize reward while staying close to the original model\n",
    "\n",
    "---\n",
    "\n",
    "**Next notebooks**: DQN (value-based deep RL) and PPO (the full algorithm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
