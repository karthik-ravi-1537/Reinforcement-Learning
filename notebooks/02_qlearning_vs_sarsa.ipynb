{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning vs SARSA: On-Policy vs Off-Policy\n",
    "\n",
    "**Goal**: Understand the difference between on-policy and off-policy learning through hands-on comparison.\n",
    "\n",
    "## Key Concepts\n",
    "- **Off-policy (Q-learning)**: Learn about the *optimal* policy while following an exploratory policy\n",
    "- **On-policy (SARSA)**: Learn about the policy *you're actually following*\n",
    "\n",
    "## The Core Difference\n",
    "\n",
    "| Algorithm | Update Target | Learns About |\n",
    "|-----------|--------------|---------------|\n",
    "| Q-Learning | `r + γ max Q(s', a')` | Optimal policy |\n",
    "| SARSA | `r + γ Q(s', a')` where a' is the actual next action | Current policy |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import our agents\n",
    "from rl_lab.agents import QLearningAgent, SarsaAgent, ExpectedSarsaAgent\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Algorithms\n",
    "\n",
    "### Q-Learning (Off-Policy)\n",
    "```\n",
    "Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') - Q(s, a)]\n",
    "```\n",
    "\n",
    "- Uses **max** over next actions → learns optimal Q regardless of behavior\n",
    "- Can learn from old data, demonstrations, other agents\n",
    "- May overestimate Q-values (optimism)\n",
    "\n",
    "### SARSA (On-Policy)\n",
    "```\n",
    "Q(s, a) ← Q(s, a) + α [r + γ Q(s', a') - Q(s, a)]\n",
    "```\n",
    "\n",
    "- Uses the **actual** next action a' → learns Q for the current policy\n",
    "- More conservative: accounts for exploration \"mistakes\"\n",
    "- Better for continuing tasks where exploration has real costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, n_episodes=5000, max_steps=100):\n",
    "    \"\"\"\n",
    "    Train an agent and track metrics.\n",
    "    \n",
    "    Args:\n",
    "        agent: RL agent with select_action and update methods\n",
    "        env: Gymnasium environment\n",
    "        n_episodes: Number of training episodes\n",
    "        max_steps: Max steps per episode\n",
    "    \n",
    "    Returns:\n",
    "        Dict with training history\n",
    "    \"\"\"\n",
    "    rewards_history = []\n",
    "    success_history = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        if hasattr(agent, 'reset_episode'):\n",
    "            agent.reset_episode()\n",
    "        \n",
    "        total_reward = 0\n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        rewards_history.append(total_reward)\n",
    "        success_history.append(1 if total_reward > 0 else 0)\n",
    "    \n",
    "    return {\n",
    "        'rewards': rewards_history,\n",
    "        'successes': success_history,\n",
    "        'q_table': agent.q_table.copy()\n",
    "    }\n",
    "\n",
    "def evaluate_agent(agent, env, n_episodes=1000):\n",
    "    \"\"\"Evaluate agent without exploration.\"\"\"\n",
    "    successes = 0\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.select_action(state, training=False)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            if reward > 0:\n",
    "                successes += 1\n",
    "                break\n",
    "    return successes / n_episodes\n",
    "\n",
    "def running_mean(data, window=100):\n",
    "    \"\"\"Compute running mean with given window.\"\"\"\n",
    "    cumsum = np.cumsum(np.insert(data, 0, 0))\n",
    "    return (cumsum[window:] - cumsum[:-window]) / window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Experiment: FrozenLake Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slippery FrozenLake\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(f\"Environment: FrozenLake (slippery=True)\")\n",
    "print(f\"States: {n_states}, Actions: {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "config = {\n",
    "    'n_states': n_states,\n",
    "    'n_actions': n_actions,\n",
    "    'learning_rate': 0.1,\n",
    "    'gamma': 0.99,\n",
    "    'epsilon': 1.0,\n",
    "    'epsilon_min': 0.01,\n",
    "    'epsilon_decay': 0.9995,\n",
    "}\n",
    "\n",
    "n_episodes = 10000\n",
    "\n",
    "# Train all three agents\n",
    "print(\"Training Q-Learning...\")\n",
    "q_agent = QLearningAgent(**config, seed=42)\n",
    "q_results = train_agent(q_agent, env, n_episodes=n_episodes)\n",
    "q_eval = evaluate_agent(q_agent, env)\n",
    "print(f\"  Final eval: {q_eval:.1%}\")\n",
    "\n",
    "print(\"\\nTraining SARSA...\")\n",
    "sarsa_agent = SarsaAgent(**config, seed=42)\n",
    "sarsa_results = train_agent(sarsa_agent, env, n_episodes=n_episodes)\n",
    "sarsa_eval = evaluate_agent(sarsa_agent, env)\n",
    "print(f\"  Final eval: {sarsa_eval:.1%}\")\n",
    "\n",
    "print(\"\\nTraining Expected SARSA...\")\n",
    "exp_sarsa_agent = ExpectedSarsaAgent(**config, seed=42)\n",
    "exp_sarsa_results = train_agent(exp_sarsa_agent, env, n_episodes=n_episodes)\n",
    "exp_sarsa_eval = evaluate_agent(exp_sarsa_agent, env)\n",
    "print(f\"  Final eval: {exp_sarsa_eval:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Success rate over training\n",
    "ax = axes[0]\n",
    "window = 100\n",
    "ax.plot(running_mean(q_results['successes'], window), label='Q-Learning', alpha=0.8)\n",
    "ax.plot(running_mean(sarsa_results['successes'], window), label='SARSA', alpha=0.8)\n",
    "ax.plot(running_mean(exp_sarsa_results['successes'], window), label='Expected SARSA', alpha=0.8)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Success Rate (100-ep rolling)')\n",
    "ax.set_title('Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Final evaluation comparison\n",
    "ax = axes[1]\n",
    "agents = ['Q-Learning', 'SARSA', 'Expected SARSA']\n",
    "scores = [q_eval, sarsa_eval, exp_sarsa_eval]\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6']\n",
    "bars = ax.bar(agents, scores, color=colors)\n",
    "ax.set_ylabel('Success Rate')\n",
    "ax.set_title('Final Evaluation (1000 episodes, no exploration)')\n",
    "ax.set_ylim(0, 1)\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, score + 0.02, f'{score:.1%}', \n",
    "            ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Visualize Learned Q-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(q_table, title=\"Policy\"):\n",
    "    \"\"\"Visualize the greedy policy from Q-table.\"\"\"\n",
    "    action_symbols = ['←', '↓', '→', '↑']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Get max Q-value for each state (for coloring)\n",
    "    v = np.max(q_table, axis=1).reshape(4, 4)\n",
    "    im = ax.imshow(v, cmap='YlOrRd')\n",
    "    \n",
    "    # Add arrows for best action\n",
    "    for s in range(16):\n",
    "        i, j = s // 4, s % 4\n",
    "        best_action = np.argmax(q_table[s])\n",
    "        ax.text(j, i, action_symbols[best_action], ha='center', va='center', \n",
    "                fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Mark holes and goal\n",
    "    holes = [(1, 1), (1, 3), (2, 3), (3, 0)]\n",
    "    for (i, j) in holes:\n",
    "        ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=True, \n",
    "                                    facecolor='lightblue', edgecolor='blue', linewidth=2))\n",
    "        ax.text(j, i, 'H', ha='center', va='center', fontsize=16, color='blue')\n",
    "    \n",
    "    ax.add_patch(plt.Rectangle((3-0.5, 3-0.5), 1, 1, fill=True,\n",
    "                                facecolor='lightgreen', edgecolor='green', linewidth=2))\n",
    "    ax.text(3, 3, 'G', ha='center', va='center', fontsize=16, color='green')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_yticks(range(4))\n",
    "    plt.colorbar(im, label='V(s) = max Q(s,a)')\n",
    "    return fig\n",
    "\n",
    "fig = visualize_policy(q_results['q_table'], \"Q-Learning Policy\")\n",
    "plt.show()\n",
    "\n",
    "fig = visualize_policy(sarsa_results['q_table'], \"SARSA Policy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. The Cliff Walking Example\n",
    "\n",
    "The classic example showing on-policy vs off-policy difference.\n",
    "\n",
    "```\n",
    "Start: S        Goal: G\n",
    "Grid with cliff at bottom (falling = -100 reward)\n",
    "\n",
    "S . . . . . . . . . . G\n",
    "C C C C C C C C C C C C  (Cliff)\n",
    "```\n",
    "\n",
    "- **Q-Learning**: Learns the optimal path (right along the cliff edge) but during training, exploration causes falls\n",
    "- **SARSA**: Learns a safer path (away from cliff) because it accounts for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CliffWalking environment\n",
    "cliff_env = gym.make('CliffWalking-v0')\n",
    "\n",
    "print(f\"CliffWalking: {cliff_env.observation_space.n} states, {cliff_env.action_space.n} actions\")\n",
    "print(\"Actions: 0=Up, 1=Right, 2=Down, 3=Left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on CliffWalking\n",
    "cliff_config = {\n",
    "    'n_states': cliff_env.observation_space.n,\n",
    "    'n_actions': cliff_env.action_space.n,\n",
    "    'learning_rate': 0.5,\n",
    "    'gamma': 1.0,  # No discounting for this episodic task\n",
    "    'epsilon': 0.1,  # Fixed exploration\n",
    "    'epsilon_min': 0.1,\n",
    "    'epsilon_decay': 1.0,  # No decay\n",
    "}\n",
    "\n",
    "n_eps = 500\n",
    "\n",
    "print(\"Training Q-Learning on Cliff...\")\n",
    "q_cliff = QLearningAgent(**cliff_config, seed=42)\n",
    "q_cliff_results = train_agent(q_cliff, cliff_env, n_episodes=n_eps, max_steps=200)\n",
    "\n",
    "print(\"Training SARSA on Cliff...\")\n",
    "sarsa_cliff = SarsaAgent(**cliff_config, seed=42)\n",
    "sarsa_cliff_results = train_agent(sarsa_cliff, cliff_env, n_episodes=n_eps, max_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training rewards\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "window = 20\n",
    "ax.plot(running_mean(q_cliff_results['rewards'], window), label='Q-Learning', alpha=0.8)\n",
    "ax.plot(running_mean(sarsa_cliff_results['rewards'], window), label='SARSA', alpha=0.8)\n",
    "ax.axhline(y=-13, color='green', linestyle='--', label='Optimal path (-13)', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward (20-ep rolling mean)')\n",
    "ax.set_title('CliffWalking: Q-Learning vs SARSA')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-150, 0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nQ-Learning avg reward (last 100): {np.mean(q_cliff_results['rewards'][-100:]):.1f}\")\n",
    "print(f\"SARSA avg reward (last 100): {np.mean(sarsa_cliff_results['rewards'][-100:]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cliff_policy(q_table):\n",
    "    \"\"\"Visualize CliffWalking policy.\"\"\"\n",
    "    action_symbols = ['↑', '→', '↓', '←']\n",
    "    \n",
    "    # CliffWalking is 4x12 grid\n",
    "    policy = np.argmax(q_table, axis=1).reshape(4, 12)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    \n",
    "    # Color by max Q-value\n",
    "    v = np.max(q_table, axis=1).reshape(4, 12)\n",
    "    im = ax.imshow(v, cmap='RdYlGn', aspect='equal')\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(12):\n",
    "            s = i * 12 + j\n",
    "            # Mark cliff\n",
    "            if i == 3 and 0 < j < 11:\n",
    "                ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=True,\n",
    "                            facecolor='black', edgecolor='red', linewidth=1))\n",
    "                ax.text(j, i, 'C', ha='center', va='center', color='red', fontsize=10)\n",
    "            # Mark start\n",
    "            elif i == 3 and j == 0:\n",
    "                ax.text(j, i, 'S', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "            # Mark goal\n",
    "            elif i == 3 and j == 11:\n",
    "                ax.text(j, i, 'G', ha='center', va='center', fontsize=12, fontweight='bold', color='green')\n",
    "            else:\n",
    "                ax.text(j, i, action_symbols[policy[i, j]], ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    ax.set_xticks(range(12))\n",
    "    ax.set_yticks(range(4))\n",
    "    plt.colorbar(im)\n",
    "    return fig\n",
    "\n",
    "print(\"Q-Learning Policy (optimal but risky):\")\n",
    "fig = visualize_cliff_policy(q_cliff_results['q_table'])\n",
    "plt.title(\"Q-Learning: Learns optimal path along cliff edge\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSARSA Policy (safer):\")\n",
    "fig = visualize_cliff_policy(sarsa_cliff_results['q_table'])\n",
    "plt.title(\"SARSA: Learns safer path away from cliff\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Key Takeaways\n",
    "\n",
    "### Q-Learning (Off-Policy)\n",
    "- Learns optimal Q-values regardless of behavior policy\n",
    "- Better for finding the theoretically best policy\n",
    "- Can be more sample efficient (reuse data)\n",
    "- May perform worse during training if exploration is costly\n",
    "\n",
    "### SARSA (On-Policy)\n",
    "- Learns Q-values for the policy being followed\n",
    "- More conservative and \"safer\" in risky environments\n",
    "- Better when exploration has real costs\n",
    "- Generally more stable training\n",
    "\n",
    "### Expected SARSA\n",
    "- Best of both worlds: on-policy style with reduced variance\n",
    "- Often outperforms both Q-learning and SARSA\n",
    "- Computationally more expensive per update\n",
    "\n",
    "### When to Use What\n",
    "| Scenario | Recommended |\n",
    "|----------|-------------|\n",
    "| Safe simulation, want optimal policy | Q-Learning |\n",
    "| Real-world with dangerous states | SARSA |\n",
    "| Best average performance | Expected SARSA |\n",
    "| Off-policy data (demos, replays) | Q-Learning |\n",
    "\n",
    "---\n",
    "## Next Up: Deep Q-Networks (DQN)\n",
    "\n",
    "Tabular methods don't scale. What if we have millions of states?\n",
    "\n",
    "→ Use neural networks to approximate Q(s, a)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
